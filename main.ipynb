{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé≠ Identity-Aware Segmentation with SAM 3 & InsightFace\n",
    "\n",
    "**‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:** ‡∏£‡∏∞‡∏ö‡∏ö Segmentation ‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ß‡∏á IVE\n",
    "\n",
    "**‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ:**\n",
    "- SAM 3 (Segment Anything Model 3) - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Segmentation\n",
    "- InsightFace - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Face Recognition\n",
    "- RTX 6000 (48GB VRAM) Optimized\n",
    "\n",
    "**‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE 6 ‡∏Ñ‡∏ô:** Wonyoung, Yujin, Gaeul, Liz, Leeseo, Rei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Environment Setup](#section-1)\n",
    "2. [Face Embedding Database](#section-2)\n",
    "3. [Identity Matching](#section-3)\n",
    "4. [SAM 3 Engine](#section-4)\n",
    "5. [Integration Pipeline](#section-5)\n",
    "6. [Gradio UI](#section-6)\n",
    "7. [Video Inference](#section-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-1'></a>\n",
    "## Section 1: Environment Setup üîß\n",
    "\n",
    "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RTX 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1.1: Install Dependencies\n",
    "# =============================================================================\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch 2.5.1 ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö CUDA 12.1\n",
    "!pip install -q torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Face Recognition ‡πÅ‡∏•‡∏∞ Segmentation libraries\n",
    "!pip install -q insightface onnxruntime-gpu opencv-python Pillow\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á UI ‡πÅ‡∏•‡∏∞ Utilities\n",
    "!pip install -q gradio matplotlib scikit-learn scipy\n",
    "!pip install -q huggingface-hub transformers accelerate\n",
    "\n",
    "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Video Processing\n",
    "!pip install -q tqdm imageio imageio-ffmpeg av\n",
    "\n",
    "print(\"‚úÖ Dependencies ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ SAM 3 directory ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\n",
      "/root/SEGMENTATION_IVE/sam3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SEGMENTATION_IVE/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/SEGMENTATION_IVE\n",
      "‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SAM 3 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1.2: Clone ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SAM 3\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå sam3 ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "if not os.path.exists(\"sam3\"):\n",
    "    print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á clone SAM 3 repository...\")\n",
    "    !git clone https://github.com/facebookresearch/sam3.git\n",
    "    print(\"‚úÖ Clone SAM 3 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
    "else:\n",
    "    print(\"üìÅ SAM 3 directory ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\")\n",
    "\n",
    "# ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô directory ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
    "%cd sam3\n",
    "!pip install -q -e \".[notebooks]\"\n",
    "%cd ..\n",
    "\n",
    "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SAM 3 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà HuggingFace Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
      "   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://huggingface.co/settings/tokens\n",
      "   - ‡∏™‡∏£‡πâ‡∏≤‡∏á token ‡πÉ‡∏´‡∏°‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ access ‡∏ñ‡∏∂‡∏á SAM 3)\n",
      "   - ‡∏ß‡∏≤‡∏á token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:\n",
      "‚úÖ Login HuggingFace ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1.3: HuggingFace Login\n",
    "# =============================================================================\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"üîë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà HuggingFace Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\")\n",
    "print(\"   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://huggingface.co/settings/tokens\")\n",
    "print(\"   - ‡∏™‡∏£‡πâ‡∏≤‡∏á token ‡πÉ‡∏´‡∏°‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ access ‡∏ñ‡∏∂‡∏á SAM 3)\")\n",
    "print(\"   - ‡∏ß‡∏≤‡∏á token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:\")\n",
    "login(token=\"\")\n",
    "\n",
    "print(\"‚úÖ Login HuggingFace ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üñ•Ô∏è  GPU Information\n",
      "============================================================\n",
      "‚úÖ GPU: NVIDIA RTX 6000 Ada Generation\n",
      "üìå CUDA Version: 12.1\n",
      "üìå Total VRAM: 47.37 GB\n",
      "üìå Compute Capability: 8.9\n",
      "‚úÖ bfloat16 supported!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1.4: Verify GPU ‡πÅ‡∏•‡∏∞ CUDA\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üñ•Ô∏è  GPU Information\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    \n",
    "    cuda_version = torch.version.cuda\n",
    "    print(f\"üìå CUDA Version: {cuda_version}\")\n",
    "    \n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"üìå Total VRAM: {total_memory:.2f} GB\")\n",
    "    \n",
    "    major = torch.cuda.get_device_capability(0)[0]\n",
    "    minor = torch.cuda.get_device_capability(0)[1]\n",
    "    print(f\"üìå Compute Capability: {major}.{minor}\")\n",
    "    \n",
    "    if major >= 8:\n",
    "        print(\"‚úÖ bfloat16 supported!\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1.5: Import All Libraries\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Face Recognition\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# SAM 3 (Facebook Hugging Face)\n",
    "from transformers import Sam3Model, Sam3Processor\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "# Utilities\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-2'></a>\n",
    "## Section 2: Face Embedding Database üë§\n",
    "\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Face Embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE 6 ‡∏Ñ‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î InsightFace model (buffalo_l)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./insightface_models/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./insightface_models/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./insightface_models/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./insightface_models/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\n",
      "find model: ./insightface_models/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "‚úÖ InsightFace model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SEGMENTATION_IVE/.venv/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Yujin: 16 embeddings\n",
      "[OK] Wonyoung: 22 embeddings\n",
      "[OK] Gaeul: 18 embeddings\n",
      "[OK] Liz: 25 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Leeseo: 24 embeddings\n",
      "[OK] Rei: 17 embeddings\n",
      "\n",
      "‚úÖ Face Embedding Database ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô! (6 ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 2: Face Embedding Database Creation\n",
    "# =============================================================================\n",
    "\n",
    "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\n",
    "MEMBER_MAPPING = {\n",
    "    \"An_Yujin\": \"Yujin\",\n",
    "    \"Jang_Wonyoung\": \"Wonyoung\",\n",
    "    \"Kim_Gaeul\": \"Gaeul\",\n",
    "    \"Kim_Jiwon\": \"Liz\",\n",
    "    \"Lee_Hyunseo\": \"Leeseo\",\n",
    "    \"Naoi_Rei\": \"Rei\"\n",
    "}\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î InsightFace\n",
    "print(\"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î InsightFace model (buffalo_l)...\")\n",
    "face_analyzer = FaceAnalysis(\n",
    "    name='buffalo_l',\n",
    "    root='./insightface_models',\n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "face_analyzer.prepare(ctx_id=0, det_size=(640, 640))\n",
    "print(\"‚úÖ InsightFace model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n",
    "\n",
    "def normalize_embedding(embedding):\n",
    "    \"\"\"‡∏ó‡∏≥ L2 Normalization ‡∏ö‡∏ô face embedding\"\"\"\n",
    "    norm = np.linalg.norm(embedding)\n",
    "    if norm < 1e-10:\n",
    "        return embedding\n",
    "    return embedding / norm\n",
    "\n",
    "def create_embedding_database(dataset_path, face_analyzer, member_mapping, max_imgs_per_id=50, det_thresh=0.50):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• face embeddings ‡πÅ‡∏ö‡∏ö multi-embedding per person\n",
    "    ‡πÄ‡∏Å‡πá‡∏ö‡∏´‡∏•‡∏≤‡∏¢ embeddings ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏∏‡∏°/‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤\n",
    "    \"\"\"\n",
    "    embeddings_db = {}\n",
    "    \n",
    "    for folder_name, member_name in member_mapping.items():\n",
    "        member_path = Path(dataset_path) / folder_name\n",
    "        \n",
    "        if not member_path.exists():\n",
    "            print(f\"[WARN] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {member_path}\")\n",
    "            continue\n",
    "        \n",
    "        image_files = sorted(list(member_path.glob('*.jpg')) + list(member_path.glob('*.png')))[:max_imgs_per_id]\n",
    "        member_embeddings = []\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            img_bgr = cv2.imread(str(img_path))\n",
    "            if img_bgr is None:\n",
    "                continue\n",
    "            \n",
    "            faces = face_analyzer.get(img_bgr)\n",
    "            if len(faces) > 0:\n",
    "                # Filter by detection score and pick largest\n",
    "                valid_faces = [f for f in faces if f.det_score >= det_thresh]\n",
    "                if not valid_faces:\n",
    "                    valid_faces = faces\n",
    "                face = max(valid_faces, key=lambda f: (f.bbox[2]-f.bbox[0])*(f.bbox[3]-f.bbox[1]))\n",
    "                # Use normed_embedding directly from InsightFace\n",
    "                emb = face.normed_embedding\n",
    "                if emb is not None and len(emb) > 0:\n",
    "                    member_embeddings.append(emb)\n",
    "        \n",
    "        if len(member_embeddings) > 0:\n",
    "            # Store all embeddings as a matrix [N, 512] for better pose coverage\n",
    "            embeddings_matrix = np.stack(member_embeddings, axis=0)\n",
    "            # Normalize again to ensure all embeddings are unit vectors\n",
    "            embeddings_matrix = np.stack([normalize_embedding(e) for e in embeddings_matrix], axis=0)\n",
    "            embeddings_db[member_name] = embeddings_matrix\n",
    "            print(f\"[OK] {member_name}: {len(member_embeddings)} embeddings\")\n",
    "        else:\n",
    "            print(f\"[WARN] {member_name}: no embeddings extracted\")\n",
    "    \n",
    "    return embeddings_db\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings database\n",
    "embeddings_db = create_embedding_database(\"Dataset\", face_analyzer, MEMBER_MAPPING)\n",
    "print(f\"\\n‚úÖ Face Embedding Database ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô! ({len(embeddings_db)} ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-3'></a>\n",
    "## Section 3: Identity Matching üéØ\n",
    "\n",
    "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE ‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û\n",
    "‡πÉ‡∏ä‡πâ Cosine Similarity ‡πÅ‡∏•‡∏∞ Hungarian Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SECTION 3: Identity Matching Functions\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 embeddings\"\"\"\n",
    "    return np.dot(embedding1, embedding2)\n",
    "\n",
    "\n",
    "def best_match(embedding, embeddings_db, online_db=None, threshold=0.40):\n",
    "    \"\"\"\n",
    "    ‡∏´‡∏≤ match ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å embeddings database\n",
    "    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á single embedding ‡πÅ‡∏•‡∏∞ multi-embedding gallery\n",
    "    \"\"\"\n",
    "    best_name = \"Unknown\"\n",
    "    best_score = -1.0\n",
    "    \n",
    "    for name, ref_emb in embeddings_db.items():\n",
    "        if isinstance(ref_emb, np.ndarray) and ref_emb.ndim == 2:\n",
    "            # Multi-embedding: use max similarity across all stored embeddings\n",
    "            score = float(np.max(ref_emb @ embedding))\n",
    "        else:\n",
    "            # Single embedding fallback\n",
    "            score = cosine_similarity(embedding, ref_emb)\n",
    "        \n",
    "        # Also check online gallery if available\n",
    "        if online_db and name in online_db and len(online_db[name]) > 0:\n",
    "            online_mat = np.stack(list(online_db[name]), axis=0)\n",
    "            online_score = float(np.max(online_mat @ embedding))\n",
    "            score = max(score, online_score)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "    \n",
    "    if best_score < threshold:\n",
    "        best_name = \"Unknown\"\n",
    "    \n",
    "    return best_name, best_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Advanced Tracking Classes (from ive_face_recognition.ipynb)\n",
    "from collections import deque, Counter, defaultdict\n",
    "\n",
    "\n",
    "class Track:\n",
    "    \"\"\"Track class with temporal smoothing for stable identification\"\"\"\n",
    "    \n",
    "    def __init__(self, track_id, bbox, embedding, name, sim, bbox_momentum=0.60):\n",
    "        self.id = track_id\n",
    "        self.bbox = bbox.astype(float)\n",
    "        self.embedding = embedding\n",
    "        self.name_history = deque([name], maxlen=10)\n",
    "        self.sim_history = deque([sim], maxlen=10)\n",
    "        self.lost = 0\n",
    "        self.last_update = 0\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "    \n",
    "    def update(self, bbox, embedding, name, sim):\n",
    "        # Smooth bbox to reduce jitter\n",
    "        self.bbox = self.bbox_momentum * bbox + (1 - self.bbox_momentum) * self.bbox\n",
    "        # Exponential moving average of embedding\n",
    "        self.embedding = normalize_embedding(0.7 * self.embedding + 0.3 * embedding)\n",
    "        self.name_history.append(name)\n",
    "        self.sim_history.append(sim)\n",
    "        self.lost = 0\n",
    "        self.last_update = 0\n",
    "    \n",
    "    @property\n",
    "    def stable_name(self):\n",
    "        \"\"\"Get most common name from history (majority voting)\"\"\"\n",
    "        counts = Counter([n for n in self.name_history if n != \"Unknown\"])\n",
    "        if not counts:\n",
    "            return \"Unknown\"\n",
    "        return counts.most_common(1)[0][0]\n",
    "    \n",
    "    @property\n",
    "    def stable_sim(self):\n",
    "        \"\"\"Get average similarity from history\"\"\"\n",
    "        if not self.sim_history:\n",
    "            return 0.0\n",
    "        return float(np.mean(self.sim_history))\n",
    "\n",
    "\n",
    "class AdvancedTracker:\n",
    "    \"\"\"Advanced tracker combining IoU and embedding similarity\"\"\"\n",
    "    \n",
    "    def __init__(self, iou_thr=0.5, embed_thr=0.40, max_lost=5, sim_update_thr=0.42, bbox_momentum=0.65):\n",
    "        self.iou_thr = iou_thr\n",
    "        self.embed_thr = embed_thr\n",
    "        self.max_lost = max_lost\n",
    "        self.sim_update_thr = sim_update_thr\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "        self.tracks = []\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def compute_iou(self, boxA, boxB):\n",
    "        \"\"\"Compute IoU between two boxes\"\"\"\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxB[3], boxB[3])\n",
    "        inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "        areaA = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
    "        areaB = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
    "        return inter / (areaA + areaB - inter + 1e-6)\n",
    "    \n",
    "    def step(self, detections):\n",
    "        \"\"\"\n",
    "        Update tracker with new detections\n",
    "        detections: list of dict with 'bbox', 'embedding', 'name', 'sim'\n",
    "        \"\"\"\n",
    "        assigned = set()\n",
    "        \n",
    "        for det in detections:\n",
    "            # Skip weak matches to avoid label flicker\n",
    "            if det['sim'] < self.sim_update_thr:\n",
    "                continue\n",
    "            \n",
    "            best_score = -1\n",
    "            best_track = None\n",
    "            \n",
    "            for trk in self.tracks:\n",
    "                iou_score = self.compute_iou(det['bbox'], trk.bbox)\n",
    "                sim_score = cosine_similarity(det['embedding'], trk.embedding)\n",
    "                score = iou_score + sim_score\n",
    "                \n",
    "                if iou_score > self.iou_thr and sim_score > self.embed_thr and score > best_score:\n",
    "                    best_score = score\n",
    "                    best_track = trk\n",
    "            \n",
    "            if best_track:\n",
    "                best_track.update(det['bbox'], det['embedding'], det['name'], det['sim'])\n",
    "                assigned.add(best_track.id)\n",
    "            else:\n",
    "                # Create new track\n",
    "                trk = Track(self.next_id, det['bbox'], det['embedding'], det['name'], det['sim'], \n",
    "                           bbox_momentum=self.bbox_momentum)\n",
    "                self.tracks.append(trk)\n",
    "                self.next_id += 1\n",
    "        \n",
    "        # Mark lost tracks\n",
    "        alive = []\n",
    "        for trk in self.tracks:\n",
    "            if trk.id not in assigned:\n",
    "                trk.lost += 1\n",
    "            if trk.lost <= self.max_lost:\n",
    "                alive.append(trk)\n",
    "        self.tracks = alive\n",
    "        \n",
    "        return self.tracks\n",
    "\n",
    "\n",
    "\n",
    "def hungarian_matching(faces, embeddings_db, threshold=0.45):\n",
    "    \"\"\"\n",
    "    ‡πÉ‡∏ä‡πâ Hungarian Algorithm ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏µ‡πà optimal\n",
    "    ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "    \"\"\"\n",
    "    if len(faces) == 0 or len(embeddings_db) == 0:\n",
    "        return []\n",
    "    \n",
    "    member_names = list(embeddings_db.keys())\n",
    "    member_embeddings = [embeddings_db[name] for name in member_names]\n",
    "    \n",
    "    num_faces = len(faces)\n",
    "    num_members = len(member_names)\n",
    "    \n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Similarity Matrix\n",
    "    similarity_matrix = np.zeros((num_faces, num_members))\n",
    "    \n",
    "    for i, face in enumerate(faces):\n",
    "        face_embedding = face.embedding / (np.linalg.norm(face.embedding) + 1e-10)\n",
    "        for j, member_emb in enumerate(member_embeddings):\n",
    "            similarity = cosine_similarity(face_embedding, member_emb)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    # ‡πÉ‡∏ä‡πâ Hungarian Algorithm\n",
    "    cost_matrix = -similarity_matrix\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    matches = [None] * num_faces\n",
    "    \n",
    "    for face_idx, member_idx in zip(row_ind, col_ind):\n",
    "        similarity = similarity_matrix[face_idx, member_idx]\n",
    "        if similarity >= threshold:\n",
    "            member_name = member_names[member_idx]\n",
    "            matches[face_idx] = (face_idx, member_name, similarity)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def identify_all_members(image_bgr, face_analyzer, embeddings_db, threshold=0.40, det_thresh=0.45, online_db=None):\n",
    "    \"\"\"\n",
    "    ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏û ‡∏û‡∏£‡πâ‡∏≠‡∏° detection score filtering\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    faces = face_analyzer.get(image_bgr)\n",
    "    \n",
    "    # Filter by detection score\n",
    "    faces = [f for f in faces if f.det_score >= det_thresh]\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        print(\"[INFO] ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏£‡∏≠‡∏á det_score)\")\n",
    "        return results\n",
    "    \n",
    "    print(f\"[INFO] ‡∏û‡∏ö {len(faces)} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (det_score >= {det_thresh})\")\n",
    "    \n",
    "    for face in faces:\n",
    "        bbox = face.bbox.astype(int).tolist()\n",
    "        # Use normed_embedding directly\n",
    "        emb = face.normed_embedding if face.normed_embedding is not None else face.embedding\n",
    "        emb = normalize_embedding(emb)\n",
    "        \n",
    "        # Use best_match instead of hungarian_matching for multi-embedding support\n",
    "        name, sim = best_match(emb, embeddings_db, online_db=online_db, threshold=threshold)\n",
    "        \n",
    "        results.append({\n",
    "            'name': name,\n",
    "            'bbox': bbox,\n",
    "            'similarity': float(sim),\n",
    "            'det_score': float(face.det_score),\n",
    "            'embedding': emb  # Include embedding for tracking\n",
    "        })\n",
    "        \n",
    "        status = \"[OK]\" if name != \"Unknown\" else \"[?]\"\n",
    "        print(f\"   {status} {name}: sim={sim:.4f}, det={face.det_score:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-4'></a>\n",
    "## Section 4: SAM 3 Engine ‚úÇÔ∏è\n",
    "\n",
    "‡πÇ‡∏´‡∏•‡∏î SAM 3 ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß SAM 3 Engine: Using device = cuda\n",
      "\n",
      "üì¶ Loading SAM 3 Model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SAM 3 Model loaded successfully!\n",
      "‚úÖ SAM 3 segmentation functions ready!\n",
      "   - segment_by_box(): ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å bounding box\n",
      "   - segment_by_points(): ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å points\n",
      "   - refine_mask_with_points(): ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á mask ‡∏î‡πâ‡∏ß‡∏¢ points\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 4: SAM 3 Engine (Improved)\n",
    "# =============================================================================\n",
    "\n",
    "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ precision ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RTX 6000\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß SAM 3 Engine: Using device = {DEVICE}\")\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î SAM 3 Model ‡∏à‡∏≤‡∏Å Hugging Face repo: facebook/sam3\n",
    "print(\"\\nüì¶ Loading SAM 3 Model...\")\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sam3_repo_root = Path(\"sam3\").resolve()\n",
    "if str(sam3_repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(sam3_repo_root))\n",
    "\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "\n",
    "sam3_model = build_sam3_image_model(\n",
    "    device=DEVICE.type,\n",
    "    load_from_HF=True,\n",
    "    compile=False,\n",
    "    enable_inst_interactivity=True,\n",
    "    eval_mode=True,\n",
    "    checkpoint_path=None,\n",
    "    bpe_path=None,\n",
    "    enable_segmentation=True\n",
    ")\n",
    "sam3_processor = Sam3Processor(sam3_model)\n",
    "sam3_model.eval()\n",
    "\n",
    "print(\"‚úÖ SAM 3 Model loaded successfully!\")\n",
    "\n",
    "def segment_by_box(image_pil, box_xyxy, multimask_output=True):\n",
    "    \"\"\"\n",
    "    ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å bounding box (xyxy)\n",
    "    \n",
    "    Args:\n",
    "        image_pil: PIL Image\n",
    "        box_xyxy: [x1, y1, x2, y2]\n",
    "        multimask_output: ‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞ return ‡∏´‡∏•‡∏≤‡∏¢ mask ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏±‡∏ô‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "    \"\"\"\n",
    "    inference_state = sam3_processor.set_image(image_pil)\n",
    "    input_box = np.array(box_xyxy, dtype=np.float32)[None, :]\n",
    "\n",
    "    masks, scores, _ = sam3_model.predict_inst(\n",
    "        inference_state,\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_box,\n",
    "        multimask_output=multimask_output,\n",
    "    )\n",
    "\n",
    "    scores_np = np.asarray(scores)\n",
    "    \n",
    "    if multimask_output:\n",
    "        # Return the best mask based on score\n",
    "        best_idx = int(np.argmax(scores_np))\n",
    "        best_mask = np.asarray(masks[best_idx])\n",
    "    else:\n",
    "        best_mask = np.asarray(masks[0])\n",
    "\n",
    "    return (best_mask > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def segment_by_points(image_pil, point_coords, point_labels, multimask_output=True):\n",
    "    \"\"\"\n",
    "    Segment ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ point prompts\n",
    "    \n",
    "    Args:\n",
    "        image_pil: PIL Image\n",
    "        point_coords: [[x1, y1], [x2, y2], ...]\n",
    "        point_labels: [1, 0, ...] (1=positive, 0=negative)\n",
    "        multimask_output: ‡∏ñ‡πâ‡∏≤ True ‡∏à‡∏∞ return ‡∏´‡∏•‡∏≤‡∏¢ mask\n",
    "    \"\"\"\n",
    "    inference_state = sam3_processor.set_image(image_pil)\n",
    "    \n",
    "    point_coords_np = np.array(point_coords, dtype=np.float32)\n",
    "    point_labels_np = np.array(point_labels, dtype=np.int32)\n",
    "    \n",
    "    masks, scores, _ = sam3_model.predict_inst(\n",
    "        inference_state,\n",
    "        point_coords=point_coords_np[None, :, :],\n",
    "        point_labels=point_labels_np[None, :],\n",
    "        box=None,\n",
    "        multimask_output=multimask_output,\n",
    "    )\n",
    "    \n",
    "    scores_np = np.asarray(scores)\n",
    "    \n",
    "    if multimask_output:\n",
    "        best_idx = int(np.argmax(scores_np))\n",
    "        best_mask = np.asarray(masks[best_idx])\n",
    "    else:\n",
    "        best_mask = np.asarray(masks[0])\n",
    "    \n",
    "    return (best_mask > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def refine_mask_with_points(image_pil, initial_mask, point_coords, point_labels):\n",
    "    \"\"\"\n",
    "    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á mask ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ point prompts ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°\n",
    "    \"\"\"\n",
    "    # Find the bounding box of the initial mask\n",
    "    ys, xs = np.where(initial_mask > 0)\n",
    "    if len(xs) == 0:\n",
    "        return initial_mask\n",
    "    \n",
    "    x1, y1 = xs.min(), ys.min()\n",
    "    x2, y2 = xs.max(), ys.max()\n",
    "    \n",
    "    # Add margin\n",
    "    margin = 20\n",
    "    x1 = max(0, x1 - margin)\n",
    "    y1 = max(0, y1 - margin)\n",
    "    x2 = min(image_pil.width - 1, x2 + margin)\n",
    "    y2 = min(image_pil.height - 1, y2 + margin)\n",
    "    \n",
    "    # Segment with box + points\n",
    "    inference_state = sam3_processor.set_image(image_pil)\n",
    "    \n",
    "    input_box = np.array([x1, y1, x2, y2], dtype=np.float32)[None, :]\n",
    "    point_coords_np = np.array(point_coords, dtype=np.float32)\n",
    "    point_labels_np = np.array(point_labels, dtype=np.int32)\n",
    "    \n",
    "    masks, scores, _ = sam3_model.predict_inst(\n",
    "        inference_state,\n",
    "        point_coords=point_coords_np[None, :, :],\n",
    "        point_labels=point_labels_np[None, :],\n",
    "        box=input_box,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    scores_np = np.asarray(scores)\n",
    "    best_idx = int(np.argmax(scores_np))\n",
    "    best_mask = np.asarray(masks[best_idx])\n",
    "    \n",
    "    return (best_mask > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "print(\"‚úÖ SAM 3 segmentation functions ready!\")\n",
    "print(\"   - segment_by_box(): ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å bounding box\")\n",
    "print(\"   - segment_by_points(): ‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å points\")\n",
    "print(\"   - refine_mask_with_points(): ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á mask ‡∏î‡πâ‡∏ß‡∏¢ points\")\n",
    "\n",
    "\n",
    "\n",
    "def segment_with_negative_prompts(image_pil, target_bbox, other_face_bboxes, other_expansion=2.0):\n",
    "    \"\"\"\n",
    "    Segment target person while avoiding other people using negative point prompts.\n",
    "    \n",
    "    Args:\n",
    "        image_pil: PIL Image\n",
    "        target_bbox: [x1, y1, x2, y2] for target person body\n",
    "        other_face_bboxes: List of face bboxes for other people to avoid\n",
    "        other_expansion: Scale to expand other face bboxes for negative regions\n",
    "    \"\"\"\n",
    "    # If no other people, use simple box segmentation\n",
    "    if len(other_face_bboxes) == 0:\n",
    "        return segment_by_box(image_pil, target_bbox)\n",
    "    \n",
    "    # Check distance between target and others\n",
    "    tx1, ty1, tx2, ty2 = target_bbox\n",
    "    target_cx, target_cy = (tx1 + tx2) / 2, (ty1 + ty2) / 2\n",
    "    \n",
    "    # Filter out negative points that are too close to target\n",
    "    valid_other_faces = []\n",
    "    for face_bbox in other_face_bboxes:\n",
    "        fx1, fy1, fx2, fy2 = face_bbox\n",
    "        face_cx, face_cy = (fx1 + fx2) / 2, (fy1 + fy2) / 2\n",
    "        dist = np.sqrt((face_cx - target_cx)**2 + (face_cy - target_cy)**2)\n",
    "        min_dist = (tx2 - tx1) * 0.3\n",
    "        if dist > min_dist:\n",
    "            valid_other_faces.append(face_bbox)\n",
    "    \n",
    "    # If all other faces are too close, use simple box\n",
    "    if len(valid_other_faces) == 0:\n",
    "        return segment_by_box(image_pil, target_bbox)\n",
    "    \n",
    "    inference_state = sam3_processor.set_image(image_pil)\n",
    "    \n",
    "    # Positive: center of target body bbox\n",
    "    target_cx = (tx1 + tx2) / 2\n",
    "    target_cy = (ty1 + ty2) / 2\n",
    "    \n",
    "    point_coords = [[target_cx, target_cy]]\n",
    "    point_labels = [1]  # positive\n",
    "    \n",
    "    # Negative points: only for valid (far enough) other faces\n",
    "    for face_bbox in valid_other_faces:\n",
    "        fx1, fy1, fx2, fy2 = face_bbox\n",
    "        # Expand face bbox to estimate body region\n",
    "        fw = fx2 - fx1\n",
    "        fh = fy2 - fy1\n",
    "        face_cx = (fx1 + fx2) / 2\n",
    "        face_cy = (fy1 + fy2) / 2\n",
    "        \n",
    "        # Add negative point at center of other person's estimated body\n",
    "        neg_cx = face_cx\n",
    "        neg_cy = face_cy + fh * 1.5  # Below face (body center estimate)\n",
    "        \n",
    "        # Clamp to image bounds\n",
    "        neg_cx = max(0, min(image_pil.width - 1, neg_cx))\n",
    "        neg_cy = max(0, min(image_pil.height - 1, neg_cy))\n",
    "        \n",
    "        point_coords.append([neg_cx, neg_cy])\n",
    "        point_labels.append(0)  # negative\n",
    "    \n",
    "    input_box = np.array(target_bbox, dtype=np.float32)[None, :]\n",
    "    \n",
    "    masks, scores, _ = sam3_model.predict_inst(\n",
    "        inference_state,\n",
    "        point_coords=np.array(point_coords, dtype=np.float32)[None],\n",
    "        point_labels=np.array(point_labels, dtype=np.int32)[None],\n",
    "        box=input_box,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    best_idx = int(np.argmax(np.asarray(scores)))\n",
    "    return (np.asarray(masks[best_idx]) > 0).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-5'></a>\n",
    "## Section 5: Integration Pipeline üîó\n",
    "\n",
    "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á InsightFace ‡∏Å‡∏±‡∏ö SAM 3 - The Magic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Integration Pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5: Integration Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def face_to_body_bbox(face_bbox, img_shape, width_scale=3.0, height_top_scale=1.2, height_bottom_scale=5):\n",
    "    \"\"\"‡∏Ç‡∏¢‡∏≤‡∏¢ face bbox ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏±‡∏ß\"\"\"\n",
    "    x1, y1, x2, y2 = face_bbox.astype(float)\n",
    "    face_center_x = (x1 + x2) / 2.0\n",
    "    face_center_y = (y1 + y2) / 2.0\n",
    "    face_width = x2 - x1\n",
    "    face_height = y2 - y1\n",
    "    \n",
    "    img_h, img_w = img_shape\n",
    "    \n",
    "    half_body_width = (face_width * width_scale) / 2.0\n",
    "    body_x1 = face_center_x - half_body_width\n",
    "    body_x2 = face_center_x + half_body_width\n",
    "    \n",
    "    body_y1 = face_center_y - (face_height * height_top_scale)\n",
    "    body_y2 = face_center_y + (face_height * height_bottom_scale)\n",
    "    \n",
    "    body_x1 = max(0, body_x1)\n",
    "    body_y1 = max(0, body_y1)\n",
    "    body_x2 = min(img_w - 1, body_x2)\n",
    "    body_y2 = min(img_h - 1, body_y2)\n",
    "    \n",
    "    return np.array([body_x1, body_y1, body_x2, body_y2], dtype=np.int32)\n",
    "\n",
    "def create_overlay(image_rgb, mask, color=[0, 255, 128], alpha=0.5):\n",
    "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û overlay ‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà‡∏™‡∏µ‡∏ó‡∏±‡∏ö mask\"\"\"\n",
    "    overlay = image_rgb.copy()\n",
    "    mask_bool = mask.astype(bool)\n",
    "    color_layer = np.zeros_like(image_rgb)\n",
    "    color_layer[mask_bool] = color\n",
    "    overlay = cv2.addWeighted(overlay, 1.0, color_layer, alpha, 0)\n",
    "    return overlay\n",
    "\n",
    "def create_cutout(image_rgb, mask):\n",
    "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û cutout ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™\"\"\"\n",
    "    mask_bool = mask.astype(bool)\n",
    "    alpha_channel = (mask_bool * 255).astype(np.uint8)\n",
    "    cutout_rgba = np.dstack((image_rgb, alpha_channel))\n",
    "    return cutout_rgba\n",
    "\n",
    "def segment_member(image_bgr, member_name, similarity_threshold=0.45, use_negative_prompts=True, online_db=None):\n",
    "    \"\"\"\n",
    "    Pipeline ‡∏´‡∏•‡∏±‡∏Å: ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û + ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å ‡∏Ñ‡∏∑‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö\n",
    "    \n",
    "    Args:\n",
    "        image_bgr: Input image in BGR format\n",
    "        member_name: Name of the member to segment\n",
    "        similarity_threshold: Face recognition threshold\n",
    "        use_negative_prompts: Whether to use negative prompts to avoid other people\n",
    "    \"\"\"\n",
    "    if member_name not in embeddings_db:\n",
    "        return None, None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö embedding ‡∏Ç‡∏≠‡∏á '{member_name}'\"\n",
    "    \n",
    "    target_embedding = embeddings_db[member_name]\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = image_bgr.shape[:2]\n",
    "    \n",
    "    # Step 1: Identify all members\n",
    "    members = identify_all_members(image_bgr, face_analyzer, embeddings_db, similarity_threshold, online_db=online_db)\n",
    "    \n",
    "    target = None\n",
    "    other_face_bboxes = []\n",
    "    for m in members:\n",
    "        if m[\"name\"] == member_name:\n",
    "            target = m\n",
    "        else:\n",
    "            other_face_bboxes.append(m[\"bbox\"])  # Collect other people's face bboxes\n",
    "    \n",
    "    if target is None:\n",
    "        return None, None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö {member_name} ‡πÉ‡∏ô‡∏†‡∏≤‡∏û\"\n",
    "    \n",
    "\n",
    "    # Update online gallery for re-ID\n",
    "    if target and target[\"name\"] != \"Unknown\":\n",
    "        if online_db is None:\n",
    "            online_db = defaultdict(lambda: deque(maxlen=30))\n",
    "        online_db[target[\"name\"]].append(target[\"embedding\"].copy())\n",
    "    \n",
    "    # Step 2: Expand face bbox to body bbox\n",
    "    face_bbox = np.array(target[\"bbox\"])\n",
    "    body_bbox = face_to_body_bbox(face_bbox, (img_h, img_w))\n",
    "    \n",
    "    # Step 3: SAM 3 Segmentation with negative prompts\n",
    "    image_pil = Image.fromarray(image_rgb)\n",
    "    \n",
    "    if use_negative_prompts and len(other_face_bboxes) > 0:\n",
    "        # Use negative prompts to avoid segmenting other people\n",
    "        mask = segment_with_negative_prompts(image_pil, body_bbox.tolist(), other_face_bboxes)\n",
    "    else:\n",
    "        # Fallback to simple box-based segmentation\n",
    "        mask = segment_by_box(image_pil, body_bbox.tolist())\n",
    "    \n",
    "    # Step 4: Clean up the mask\n",
    "    mask = clean_mask(mask, kernel_size=7)\n",
    "    \n",
    "    # Step 5: Create outputs\n",
    "    overlay = create_overlay(image_rgb, mask)\n",
    "    cutout = create_cutout(image_rgb, mask)\n",
    "    \n",
    "    # Create annotated image\n",
    "    annotated = image_rgb.copy()\n",
    "    for m in members:\n",
    "        x1, y1, x2, y2 = m[\"bbox\"]\n",
    "        is_target = (m[\"name\"] == member_name)\n",
    "        color_box = (0, 255, 0) if is_target else (200, 200, 200)\n",
    "        thickness = 3 if is_target else 1\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color_box, thickness)\n",
    "        if m[\"similarity\"]:\n",
    "            label = f\"{m['name']} ({m['similarity']:.2f})\"\n",
    "            cv2.putText(annotated, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_box, 2)\n",
    "    \n",
    "    # Draw body bbox\n",
    "    bx1, by1, bx2, by2 = body_bbox\n",
    "    cv2.rectangle(annotated, (bx1, by1), (bx2, by2), (255, 0, 0), 2)\n",
    "    cv2.putText(annotated, \"Body BBox\", (bx1, by1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    \n",
    "    status = f\"‚úÖ Found {member_name} (sim={target['similarity']:.3f})\"\n",
    "    if use_negative_prompts and len(other_face_bboxes) > 0:\n",
    "        status += f\", excluded {len(other_face_bboxes)} other(s)\"\n",
    "    \n",
    "    return overlay, cutout, annotated, mask, status\n",
    "\n",
    "print(\"‚úÖ Integration Pipeline ready!\")\n",
    "\n",
    "def clean_mask(mask, kernel_size=7, min_area_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Clean up mask using morphological operations and connected component analysis.\n",
    "    \n",
    "    Args:\n",
    "        mask: Binary mask (0 or 1)\n",
    "        kernel_size: Size of morphological kernel\n",
    "        min_area_ratio: Minimum area ratio to keep (relative to image size)\n",
    "    \"\"\"\n",
    "    # Convert to uint8 if needed\n",
    "    if mask.dtype != np.uint8:\n",
    "        mask = (mask > 0).astype(np.uint8) * 255\n",
    "    else:\n",
    "        mask = (mask > 0).astype(np.uint8) * 255\n",
    "    \n",
    "    h, w = mask.shape\n",
    "    \n",
    "    # Create elliptical kernel for smoother results\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
    "    \n",
    "    # Close operation: fill small holes\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Open operation: remove small noise\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    # Keep only the largest connected component\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "    \n",
    "    if num_labels > 1:\n",
    "        # Find largest component (skip background at index 0)\n",
    "        areas = stats[1:, cv2.CC_STAT_AREA]\n",
    "        min_area = h * w * min_area_ratio\n",
    "        \n",
    "        # Filter out very small components\n",
    "        valid_components = areas >= min_area\n",
    "        if np.any(valid_components):\n",
    "            largest_idx = np.argmax(areas * valid_components) + 1\n",
    "            mask = (labels == largest_idx).astype(np.uint8) * 255\n",
    "        else:\n",
    "            # If no component meets criteria, use largest anyway\n",
    "            largest_idx = np.argmax(areas) + 1\n",
    "            mask = (labels == largest_idx).astype(np.uint8) * 255\n",
    "    \n",
    "    return (mask > 0).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5.5: Association Pipeline üëï\n",
    "\n",
    "‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö prompt ‡πÅ‡∏ö‡∏ö \"Wonyoung's shirt\" - ‡∏´‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Association Pipeline ready!\n",
      "   - segment_associated_object(): ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 'Wonyoung's shirt'\n",
      "   - segment_by_points(): ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö point-based segmentation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 5.5: Association Pipeline (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö \"Wonyoung's shirt\")\n",
    "# =============================================================================\n",
    "\n",
    "def segment_associated_object(image_bgr, member_name, object_text, similarity_threshold=0.45):\n",
    "    \"\"\"\n",
    "    ‡∏´‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏• (‡πÄ‡∏ä‡πà‡∏ô \"Wonyoung's shirt\", \"Yujin's hair\")\n",
    "    \n",
    "    Logic:\n",
    "    1. ‡∏´‡∏≤ person mask ‡∏Ç‡∏≠‡∏á‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\n",
    "    2. ‡πÉ‡∏ä‡πâ SAM 3 ‡∏Å‡∏±‡∏ö point prompts ‡∏ö‡∏£‡∏¥‡πÄ‡∏ß‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏\n",
    "    3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏°‡∏µ spatial overlap ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Å‡∏±‡∏ö person mask\n",
    "    \"\"\"\n",
    "    if member_name not in embeddings_db:\n",
    "        return None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö embedding ‡∏Ç‡∏≠‡∏á '{member_name}'\"\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w = image_bgr.shape[:2]\n",
    "    \n",
    "    # Step 1: Identify and get person mask\n",
    "    members = identify_all_members(image_bgr, face_analyzer, embeddings_db, similarity_threshold)\n",
    "    \n",
    "    target = None\n",
    "    for m in members:\n",
    "        if m[\"name\"] == member_name:\n",
    "            target = m\n",
    "            break\n",
    "    \n",
    "    if target is None:\n",
    "        return None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö {member_name} ‡πÉ‡∏ô‡∏†‡∏≤‡∏û\"\n",
    "    \n",
    "    # Step 2: Get person mask using body bbox\n",
    "    face_bbox = np.array(target[\"bbox\"])\n",
    "    body_bbox = face_to_body_bbox(face_bbox, (img_h, img_w))\n",
    "    \n",
    "    image_pil = Image.fromarray(image_rgb)\n",
    "    person_mask = segment_by_box(image_pil, body_bbox.tolist())\n",
    "    \n",
    "    # Step 3: Define region of interest based on object type\n",
    "    x1, y1, x2, y2 = body_bbox\n",
    "    \n",
    "    if \"shirt\" in object_text.lower() or \"dress\" in object_text.lower() or \"top\" in object_text.lower():\n",
    "        # Shirt is in the middle-lower part of body\n",
    "        roi_y1 = y1 + int((y2 - y1) * 0.25)  # Skip head\n",
    "        roi_y2 = y1 + int((y2 - y1) * 0.85)  # Above knees\n",
    "        roi_x1 = x1 + int((x2 - x1) * 0.1)\n",
    "        roi_x2 = x2 - int((x2 - x1) * 0.1)\n",
    "    elif \"hair\" in object_text.lower():\n",
    "        # Hair is above the face\n",
    "        roi_y1 = max(0, y1 - int((y2 - y1) * 0.8))\n",
    "        roi_y2 = y1 + int((y2 - y1) * 0.3)\n",
    "        roi_x1 = x1\n",
    "        roi_x2 = x2\n",
    "    elif \"pants\" in object_text.lower() or \"skirt\" in object_text.lower() or \"bottom\" in object_text.lower():\n",
    "        # Bottom is lower part\n",
    "        roi_y1 = y1 + int((y2 - y1) * 0.5)\n",
    "        roi_y2 = y2\n",
    "        roi_x1 = x1 + int((x2 - x1) * 0.15)\n",
    "        roi_x2 = x2 - int((x2 - x1) * 0.15)\n",
    "    elif \"shoes\" in object_text.lower() or \"foot\" in object_text.lower():\n",
    "        # Shoes at the bottom\n",
    "        roi_y1 = y1 + int((y2 - y1) * 0.8)\n",
    "        roi_y2 = y2\n",
    "        roi_x1 = x1 + int((x2 - x1) * 0.2)\n",
    "        roi_x2 = x2 - int((x2 - x1) * 0.2)\n",
    "    else:\n",
    "        # Default: use full body region\n",
    "        roi_y1 = y1\n",
    "        roi_y2 = y2\n",
    "        roi_x1 = x1\n",
    "        roi_x2 = x2\n",
    "    \n",
    "    # Clamp to image bounds\n",
    "    roi_x1 = max(0, roi_x1)\n",
    "    roi_y1 = max(0, roi_y1)\n",
    "    roi_x2 = min(img_w - 1, roi_x2)\n",
    "    roi_y2 = min(img_h - 1, roi_y2)\n",
    "    \n",
    "    # Step 4: Segment in ROI using SAM 3\n",
    "    roi_box = [roi_x1, roi_y1, roi_x2, roi_y2]\n",
    "    object_mask = segment_by_box(image_pil, roi_box)\n",
    "    \n",
    "    # Step 5: Find overlap with person mask\n",
    "    # Only keep parts of object_mask that overlap with person_mask\n",
    "    overlap_mask = object_mask & person_mask\n",
    "    \n",
    "    # Calculate overlap ratio\n",
    "    object_area = np.sum(object_mask > 0)\n",
    "    overlap_area = np.sum(overlap_mask > 0)\n",
    "    overlap_ratio = overlap_area / (object_area + 1e-10)\n",
    "    \n",
    "    # If overlap is too small, use the full object_mask in ROI\n",
    "    if overlap_ratio < 0.1:\n",
    "        print(f\"‚ö†Ô∏è Low overlap ({overlap_ratio:.2f}), using ROI-based mask\")\n",
    "        final_mask = object_mask\n",
    "    else:\n",
    "        final_mask = overlap_mask\n",
    "    \n",
    "    # Step 6: Create visualizations\n",
    "    # Person overlay (semi-transparent)\n",
    "    person_overlay = create_overlay(image_rgb, person_mask, color=[255, 105, 180], alpha=0.3)\n",
    "    \n",
    "    # Object overlay (different color)\n",
    "    object_overlay = create_overlay(person_overlay, final_mask, color=[0, 255, 128], alpha=0.6)\n",
    "    \n",
    "    # Annotated image\n",
    "    annotated = image_rgb.copy()\n",
    "    for m in members:\n",
    "        bx1, by1, bx2, by2 = m[\"bbox\"]\n",
    "        is_target = (m[\"name\"] == member_name)\n",
    "        color_box = (0, 255, 0) if is_target else (200, 200, 200)\n",
    "        thickness = 3 if is_target else 1\n",
    "        cv2.rectangle(annotated, (bx1, by1), (bx2, by2), color_box, thickness)\n",
    "        if m[\"similarity\"]:\n",
    "            label = f\"{m['name']} ({m['similarity']:.2f})\"\n",
    "            cv2.putText(annotated, label, (bx1, by1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_box, 2)\n",
    "    \n",
    "    # Draw ROI box\n",
    "    cv2.rectangle(annotated, (roi_x1, roi_y1), (roi_x2, roi_y2), (255, 0, 0), 2)\n",
    "    cv2.putText(annotated, f\"ROI: {object_text}\", (roi_x1, roi_y1-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "    \n",
    "    status = f\"‚úÖ {member_name}'s {object_text}: overlap={overlap_ratio:.2f}\"\n",
    "    \n",
    "    return annotated, object_overlay, final_mask, status\n",
    "\n",
    "\n",
    "def segment_by_points(image_pil, point_coords, point_labels):\n",
    "    \"\"\"\n",
    "    Segment ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ point prompts (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SAM 3)\n",
    "    \n",
    "    Args:\n",
    "        image_pil: PIL Image\n",
    "        point_coords: [[x1, y1], [x2, y2], ...]\n",
    "        point_labels: [1, 0, ...] (1=positive, 0=negative)\n",
    "    \"\"\"\n",
    "    inference_state = sam3_processor.set_image(image_pil)\n",
    "    \n",
    "    point_coords_np = np.array(point_coords, dtype=np.float32)\n",
    "    point_labels_np = np.array(point_labels, dtype=np.int32)\n",
    "    \n",
    "    masks, scores, _ = sam3_model.predict_inst(\n",
    "        inference_state,\n",
    "        point_coords=point_coords_np[None, :, :],\n",
    "        point_labels=point_labels_np[None, :],\n",
    "        box=None,\n",
    "        multimask_output=True,\n",
    "    )\n",
    "    \n",
    "    scores_np = np.asarray(scores)\n",
    "    best_idx = int(np.argmax(scores_np))\n",
    "    best_mask = np.asarray(masks[best_idx])\n",
    "    \n",
    "    return (best_mask > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Association Pipeline ready!\")\n",
    "print(\"   - segment_associated_object(): ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 'Wonyoung's shirt'\")\n",
    "print(\"   - segment_by_points(): ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö point-based segmentation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-6'></a>\n",
    "## Section 6: Gradio UI üé®\n",
    "\n",
    "‡∏™‡∏£‡πâ‡∏≤‡∏á Web Interface ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradio UI created!\n",
      "‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢: demo.launch(share=True)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 6: Gradio UI (Updated with Association Tab)\n",
    "# =============================================================================\n",
    "\n",
    "IVE_MEMBERS = [\"Wonyoung\", \"Yujin\", \"Gaeul\", \"Liz\", \"Leeseo\", \"Rei\"]\n",
    "\n",
    "def gradio_segment(input_image, member_name):\n",
    "    \"\"\"Gradio callback for image segmentation\"\"\"\n",
    "    if input_image is None:\n",
    "        return None, None, None, \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\"\n",
    "    \n",
    "    image_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    overlay, cutout, annotated, mask, msg = segment_member(image_bgr, member_name)\n",
    "    \n",
    "    if overlay is None:\n",
    "        return None, None, None, msg\n",
    "    \n",
    "    return annotated, overlay, cutout, msg\n",
    "\n",
    "def gradio_identify_all(input_image):\n",
    "    \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà identify ‡πÑ‡∏î‡πâ\"\"\"\n",
    "    if input_image is None:\n",
    "        return None, \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\"\n",
    "    \n",
    "    image_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "    members = identify_all_members(image_bgr, face_analyzer, embeddings_db)\n",
    "    \n",
    "    annotated = input_image.copy()\n",
    "    info_lines = []\n",
    "    \n",
    "    for m in members:\n",
    "        x1, y1, x2, y2 = m[\"bbox\"]\n",
    "        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        if m[\"similarity\"]:\n",
    "            label = f\"{m['name']} {m['similarity']:.2f}\"\n",
    "            info_lines.append(f\"{m['name']}: similarity={m['similarity']:.3f}\")\n",
    "        else:\n",
    "            label = m[\"name\"]\n",
    "        cv2.putText(annotated, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    return annotated, \"\\n\".join(info_lines)\n",
    "\n",
    "def gradio_associate(input_image, member_name, object_text):\n",
    "    \"\"\"Gradio callback for association (e.g., 'Wonyoung's shirt')\"\"\"\n",
    "    if input_image is None:\n",
    "        return None, None, \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\"\n",
    "    \n",
    "    image_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    annotated, object_overlay, object_mask, msg = segment_associated_object(\n",
    "        image_bgr, member_name, object_text\n",
    "    )\n",
    "    \n",
    "    if annotated is None:\n",
    "        return None, None, msg\n",
    "    \n",
    "    return annotated, object_overlay, msg\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface\n",
    "with gr.Blocks(title=\"IVE Segmentation\") as demo:\n",
    "    gr.Markdown(\"# üéØ IVE Member Segmentation with SAM 3\")\n",
    "    gr.Markdown(\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏° IVE ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å ‚Üí ‡∏£‡∏∞‡∏ö‡∏ö segment ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏´‡πâ\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"üîç Segment Member\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    img_input = gr.Image(label=\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\", type=\"numpy\")\n",
    "                    member_dropdown = gr.Dropdown(\n",
    "                        choices=IVE_MEMBERS,\n",
    "                        value=\"Wonyoung\",\n",
    "                        label=\"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\"\n",
    "                    )\n",
    "                    btn_segment = gr.Button(\"üîç Segment\", variant=\"primary\")\n",
    "                    status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    with gr.Row():\n",
    "                        out_identified = gr.Image(label=\"Identified\")\n",
    "                        out_overlay = gr.Image(label=\"Segmented\")\n",
    "                        out_cutout = gr.Image(label=\"Cutout\")\n",
    "            \n",
    "            btn_segment.click(\n",
    "                fn=gradio_segment,\n",
    "                inputs=[img_input, member_dropdown],\n",
    "                outputs=[out_identified, out_overlay, out_cutout, status_text]\n",
    "            )\n",
    "        \n",
    "        with gr.TabItem(\"üëï Association (e.g., Wonyoung's shirt)\"):\n",
    "            gr.Markdown(\"### ‡∏´‡∏≤‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•\")\n",
    "            gr.Markdown(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: `shirt`, `hair`, `pants`, `dress`, `shoes`\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=1):\n",
    "                    img_input_assoc = gr.Image(label=\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\", type=\"numpy\")\n",
    "                    member_dropdown_assoc = gr.Dropdown(\n",
    "                        choices=IVE_MEMBERS,\n",
    "                        value=\"Wonyoung\",\n",
    "                        label=\"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\"\n",
    "                    )\n",
    "                    object_text_input = gr.Textbox(\n",
    "                        label=\"‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (object)\",\n",
    "                        value=\"shirt\",\n",
    "                        placeholder=\"‡πÄ‡∏ä‡πà‡∏ô: shirt, hair, pants, dress\"\n",
    "                    )\n",
    "                    btn_associate = gr.Button(\"üëï Find Object\", variant=\"primary\")\n",
    "                    status_text_assoc = gr.Textbox(label=\"Status\", interactive=False)\n",
    "                \n",
    "                with gr.Column(scale=2):\n",
    "                    with gr.Row():\n",
    "                        out_annotated_assoc = gr.Image(label=\"Annotated with ROI\")\n",
    "                        out_object_overlay = gr.Image(label=\"Object Overlay\")\n",
    "            \n",
    "            btn_associate.click(\n",
    "                fn=gradio_associate,\n",
    "                inputs=[img_input_assoc, member_dropdown_assoc, object_text_input],\n",
    "                outputs=[out_annotated_assoc, out_object_overlay, status_text_assoc]\n",
    "            )\n",
    "        \n",
    "        with gr.TabItem(\"üë• Identify All\"):\n",
    "            with gr.Row():\n",
    "                img_input_all = gr.Image(label=\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\", type=\"numpy\")\n",
    "                btn_identify = gr.Button(\"üë• Identify All\", variant=\"primary\")\n",
    "            with gr.Row():\n",
    "                out_all = gr.Image(label=\"All Members\")\n",
    "                out_info = gr.Textbox(label=\"Info\", lines=8)\n",
    "            \n",
    "            btn_identify.click(\n",
    "                fn=gradio_identify_all,\n",
    "                inputs=[img_input_all],\n",
    "                outputs=[out_all, out_info]\n",
    "            )\n",
    "\n",
    "print(\"‚úÖ Gradio UI created!\")\n",
    "print(\"‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢: demo.launch(share=True)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on public URL: https://28863118bdc1d891d0.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://28863118bdc1d891d0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‡∏£‡∏±‡∏ô Gradio UI\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='section-7'></a>\n",
    "## Section 7: Video Inference üé¨ (IMPROVED)\n",
    "\n",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÅ‡∏ö‡∏ö Frame-by-Frame ‡∏û‡∏£‡πâ‡∏≠‡∏° Tracking ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß\n",
    "\n",
    "**‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏Å:**\n",
    "1. ‚úÖ **Hungarian Algorithm** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1-to-1 matching (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 1 ‡∏Ñ‡∏ô = ‡∏´‡∏•‡∏≤‡∏¢ track)\n",
    "2. ‚úÖ **Recognition threshold 0.45** (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.40) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î false positive\n",
    "3. ‚úÖ **Temporal smoothing** ‡∏î‡πâ‡∏ß‡∏¢ majority voting (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ 40% history ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô)\n",
    "4. ‚úÖ **Per-track mask smoothing** ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏û‡∏£‡∏¥‡∏ö‡∏Ç‡∏≠‡∏á segmentation\n",
    "5. ‚úÖ **Online gallery** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Re-ID ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading improved video inference module...\n",
      "‚úÖ Helper functions loaded!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SECTION 7: Improved Video Inference\n",
    "# =============================================================================\n",
    "\n",
    "from collections import deque, defaultdict, Counter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "print(\"üöÄ Loading improved video inference module...\")\n",
    "\n",
    "# Helper functions\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])\n",
    "    xB, yB = min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n",
    "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "    areaA = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
    "    areaB = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
    "    return inter / (areaA + areaB - inter + 1e-6)\n",
    "\n",
    "\n",
    "def best_match_hungarian(face_embeddings, embeddings_db, threshold=0.45):\n",
    "    \"\"\"\n",
    "    Hungarian Algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1-to-1 matching\n",
    "    ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ match ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡πÄ‡∏ä‡πà‡∏ô Wonyoung#8 ‡πÅ‡∏•‡∏∞ Wonyoung#3)\n",
    "    \"\"\"\n",
    "    if len(face_embeddings) == 0 or len(embeddings_db) == 0:\n",
    "        return []\n",
    "    \n",
    "    member_names = list(embeddings_db.keys())\n",
    "    num_faces = len(face_embeddings)\n",
    "    num_members = len(member_names)\n",
    "    \n",
    "    # Similarity Matrix\n",
    "    similarity_matrix = np.zeros((num_faces, num_members))\n",
    "    \n",
    "    for i, (idx, embedding, det_score, bbox) in enumerate(face_embeddings):\n",
    "        for j, name in enumerate(member_names):\n",
    "            ref_emb = embeddings_db[name]\n",
    "            if isinstance(ref_emb, np.ndarray) and ref_emb.ndim == 2:\n",
    "                sim = float(np.max(ref_emb @ embedding))\n",
    "            else:\n",
    "                sim = cosine_sim(embedding, ref_emb)\n",
    "            similarity_matrix[i, j] = sim\n",
    "    \n",
    "    # Hungarian Algorithm - ‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà optimal\n",
    "    cost_matrix = -similarity_matrix\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    matches = [None] * num_faces\n",
    "    for face_idx, member_idx in zip(row_ind, col_ind):\n",
    "        similarity = similarity_matrix[face_idx, member_idx]\n",
    "        if similarity >= threshold:\n",
    "            member_name = member_names[member_idx]\n",
    "            bbox = face_embeddings[face_idx][3]\n",
    "            matches[face_idx] = (face_idx, member_name, similarity, bbox)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved Track & Tracker classes loaded!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7.2: Improved Track & Tracker Classes\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedTrack:\n",
    "    \"\"\"Track with temporal smoothing ‡πÅ‡∏•‡∏∞ majority voting\"\"\"\n",
    "    \n",
    "    def __init__(self, track_id, bbox, embedding, name, sim, bbox_momentum=0.65):\n",
    "        self.id = track_id\n",
    "        self.bbox = np.array(bbox, dtype=float)\n",
    "        self.embedding = normalize_embedding(embedding)\n",
    "        \n",
    "        # Histories ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö temporal consistency\n",
    "        self.name_history = deque([name], maxlen=15)\n",
    "        self.sim_history = deque([sim], maxlen=15)\n",
    "        self.bbox_history = deque([bbox.copy()], maxlen=10)\n",
    "        \n",
    "        self.lost = 0\n",
    "        self.age = 1\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "        \n",
    "        # Fixed color per track\n",
    "        np.random.seed(track_id + 42)\n",
    "        self.color = tuple(int(x) for x in np.random.randint(50, 230, size=3))\n",
    "    \n",
    "    def update(self, bbox, embedding, name, sim):\n",
    "        # Smooth bbox\n",
    "        self.bbox = self.bbox_momentum * np.array(bbox) + (1 - self.bbox_momentum) * self.bbox\n",
    "        \n",
    "        # EMA for embedding\n",
    "        self.embedding = normalize_embedding(0.7 * self.embedding + 0.3 * normalize_embedding(embedding))\n",
    "        \n",
    "        self.name_history.append(name)\n",
    "        self.sim_history.append(sim)\n",
    "        self.bbox_history.append(bbox.copy())\n",
    "        \n",
    "        self.lost = 0\n",
    "        self.age += 1\n",
    "    \n",
    "    def mark_lost(self):\n",
    "        self.lost += 1\n",
    "        return self.lost\n",
    "    \n",
    "    @property\n",
    "    def stable_name(self):\n",
    "        \"\"\"Majority voting - ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 40% ‡∏Ç‡∏≠‡∏á history ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\"\"\"\n",
    "        if len(self.name_history) < 3:\n",
    "            valid_names = [n for n in self.name_history if n != \"Unknown\"]\n",
    "            return valid_names[-1] if valid_names else \"Unknown\"\n",
    "        \n",
    "        counts = Counter(self.name_history)\n",
    "        if not counts:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        most_common = counts.most_common(1)[0]\n",
    "        name, count = most_common\n",
    "        \n",
    "        # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 40%\n",
    "        if name != \"Unknown\" and count / len(self.name_history) >= 0.4:\n",
    "            return name\n",
    "        \n",
    "        return \"Unknown\"\n",
    "    \n",
    "    @property\n",
    "    def stable_sim(self):\n",
    "        if not self.sim_history:\n",
    "            return 0.0\n",
    "        return float(np.mean(self.sim_history))\n",
    "    \n",
    "    @property\n",
    "    def is_confirmed(self):\n",
    "        return self.age >= 3 and self.stable_name != \"Unknown\"\n",
    "\n",
    "\n",
    "class ImprovedTracker:\n",
    "    \"\"\"Tracker ‡∏î‡πâ‡∏ß‡∏¢ Hungarian matching + IoU/Embedding fusion\"\"\"\n",
    "    \n",
    "    def __init__(self, iou_thr=0.35, embed_thr=0.40, max_lost=8, \n",
    "                 sim_update_thr=0.40, bbox_momentum=0.65):\n",
    "        self.iou_thr = iou_thr\n",
    "        self.embed_thr = embed_thr\n",
    "        self.max_lost = max_lost\n",
    "        self.sim_update_thr = sim_update_thr\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "        \n",
    "        self.tracks = []\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def compute_similarity_matrix(self, tracks, detections):\n",
    "        \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cost matrix (IoU + Embedding)\"\"\"\n",
    "        n_tracks = len(tracks)\n",
    "        n_dets = len(detections)\n",
    "        \n",
    "        if n_tracks == 0 or n_dets == 0:\n",
    "            return np.zeros((n_tracks, n_dets))\n",
    "        \n",
    "        cost_matrix = np.zeros((n_tracks, n_dets))\n",
    "        \n",
    "        for i, trk in enumerate(tracks):\n",
    "            for j, det in enumerate(detections):\n",
    "                iou_score = iou(trk.bbox, det['bbox'])\n",
    "                embed_sim = cosine_sim(trk.embedding, det['embedding'])\n",
    "                # Weighted: ‡πÄ‡∏ô‡πâ‡∏ô embedding ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡πÄ‡∏£‡πá‡∏ß)\n",
    "                total_score = 0.4 * iou_score + 0.6 * embed_sim\n",
    "                cost_matrix[i, j] = total_score\n",
    "        \n",
    "        return cost_matrix\n",
    "    \n",
    "    def step(self, detections, frame_idx=0):\n",
    "        # Filter low quality detections\n",
    "        valid_detections = [d for d in detections if d['sim'] >= self.sim_update_thr]\n",
    "        \n",
    "        if len(self.tracks) == 0:\n",
    "            for det in valid_detections:\n",
    "                self._create_track(det)\n",
    "        else:\n",
    "            cost_matrix = self.compute_similarity_matrix(self.tracks, valid_detections)\n",
    "            \n",
    "            n_tracks = len(self.tracks)\n",
    "            n_dets = len(valid_detections)\n",
    "            \n",
    "            if n_tracks > 0 and n_dets > 0:\n",
    "                row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "                \n",
    "                matched_tracks = set()\n",
    "                matched_dets = set()\n",
    "                \n",
    "                for i, j in zip(row_ind, col_ind):\n",
    "                    iou_score = iou(self.tracks[i].bbox, valid_detections[j]['bbox'])\n",
    "                    embed_sim = cosine_sim(self.tracks[i].embedding, \n",
    "                                          valid_detections[j]['embedding'])\n",
    "                    \n",
    "                    if iou_score >= self.iou_thr and embed_sim >= self.embed_thr:\n",
    "                        self.tracks[i].update(\n",
    "                            valid_detections[j]['bbox'],\n",
    "                            valid_detections[j]['embedding'],\n",
    "                            valid_detections[j]['name'],\n",
    "                            valid_detections[j]['sim']\n",
    "                        )\n",
    "                        matched_tracks.add(i)\n",
    "                        matched_dets.add(j)\n",
    "                \n",
    "                # Mark lost\n",
    "                for i in range(n_tracks):\n",
    "                    if i not in matched_tracks:\n",
    "                        self.tracks[i].mark_lost()\n",
    "                \n",
    "                # Create new tracks\n",
    "                for j in range(n_dets):\n",
    "                    if j not in matched_dets:\n",
    "                        self._create_track(valid_detections[j])\n",
    "            else:\n",
    "                for trk in self.tracks:\n",
    "                    trk.mark_lost()\n",
    "                for det in valid_detections:\n",
    "                    self._create_track(det)\n",
    "        \n",
    "        # Remove dead tracks\n",
    "        self.tracks = [t for t in self.tracks if t.lost <= self.max_lost]\n",
    "        \n",
    "        return self.tracks\n",
    "    \n",
    "    def _create_track(self, detection):\n",
    "        trk = ImprovedTrack(\n",
    "            self.next_id,\n",
    "            detection['bbox'],\n",
    "            detection['embedding'],\n",
    "            detection['name'],\n",
    "            detection['sim'],\n",
    "            bbox_momentum=self.bbox_momentum\n",
    "        )\n",
    "        self.tracks.append(trk)\n",
    "        self.next_id += 1\n",
    "\n",
    "\n",
    "class TemporalMaskSmoother:\n",
    "    \"\"\"Temporal smoothing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö mask ‡πÅ‡∏ö‡∏ö per-track\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=5):\n",
    "        self.window_size = window_size\n",
    "        self.track_masks = defaultdict(lambda: deque(maxlen=window_size))\n",
    "    \n",
    "    def update(self, track_id, mask):\n",
    "        self.track_masks[track_id].append(mask.astype(np.float32))\n",
    "        \n",
    "        if len(self.track_masks[track_id]) >= 3:\n",
    "            smoothed = np.mean(self.track_masks[track_id], axis=0)\n",
    "            return (smoothed > 0.5).astype(np.uint8)\n",
    "        \n",
    "        return mask\n",
    "\n",
    "print(\"‚úÖ Improved Track & Tracker classes loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, target_member, frame_sampling=5, \n",
    "                       det_thresh=0.45, rec_thresh=0.40, tracker_iou=0.50, \n",
    "                       tracker_embed=0.36, max_lost=1, bbox_momentum=0.60):\n",
    "    \"\"\"\n",
    "    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÅ‡∏ö‡∏ö Frame-by-Frame ‡∏û‡∏£‡πâ‡∏≠‡∏° Advanced Tracking\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input video\n",
    "        output_path: Path to output video\n",
    "        target_member: Name of member to track\n",
    "        frame_sampling: Process every N frames\n",
    "        det_thresh: Face detection threshold\n",
    "        rec_thresh: Face recognition threshold\n",
    "        tracker_iou: IoU threshold for tracking\n",
    "        tracker_embed: Embedding similarity threshold for tracking\n",
    "        max_lost: Max frames to keep lost tracks\n",
    "        bbox_momentum: BBox smoothing factor (0-1)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {input_path}\")\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"[INFO] Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps / frame_sampling, (width, height))\n",
    "    \n",
    "    # Initialize IMPROVED Tracker\n",
    "    tracker = ImprovedTracker(\n",
    "        iou_thr=tracker_iou,\n",
    "        embed_thr=tracker_embed,\n",
    "        max_lost=max_lost,\n",
    "        sim_update_thr=rec_thresh * 0.9,\n",
    "        bbox_momentum=bbox_momentum\n",
    "    )\n",
    "    \n",
    "    online_gallery = defaultdict(lambda: deque(maxlen=30))\n",
    "    mask_smoother = TemporalMaskSmoother(window_size=5)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    with tqdm(total=total_frames, desc=\"Processing\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_idx % frame_sampling == 0:\n",
    "                # Step 1: Detect faces\n",
    "                faces = face_analyzer.get(frame)\n",
    "                faces = [f for f in faces if f.det_score >= det_thresh]\n",
    "                \n",
    "                # Step 2: Recognize with HUNGARIAN ALGORITHM (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!)\n",
    "                face_embeddings = []\n",
    "                for i, face in enumerate(faces):\n",
    "                    bbox = face.bbox.astype(float)\n",
    "                    emb = normalize_embedding(face.normed_embedding)\n",
    "                    face_embeddings.append((i, emb, face.det_score, bbox))\n",
    "                \n",
    "                # ‡πÉ‡∏ä‡πâ Hungarian Algorithm - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô 1 ‡∏Ñ‡∏ô = ‡∏´‡∏•‡∏≤‡∏¢ track\n",
    "                matches = best_match_hungarian(\n",
    "                    face_embeddings, embeddings_db, threshold=rec_thresh\n",
    "                )\n",
    "                \n",
    "                detections = []\n",
    "                for match in matches:\n",
    "                    if match is not None:\n",
    "                        face_idx, name, sim, bbox = match\n",
    "                        detections.append({\n",
    "                            'bbox': bbox,\n",
    "                            'embedding': face_embeddings[face_idx][1],\n",
    "                            'name': name,\n",
    "                            'sim': sim\n",
    "                        })\n",
    "                \n",
    "                # Step 3: Track with ImprovedTracker\n",
    "                tracks = tracker.step(detections, frame_idx)\n",
    "                \n",
    "                # Step 4: Visualize\n",
    "                output_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                for trk in tracks:\n",
    "                    if not trk.is_confirmed:\n",
    "                        continue\n",
    "                    \n",
    "                    # Filter by target if specified\n",
    "                    if target_member and trk.stable_name != target_member:\n",
    "                        continue\n",
    "                    \n",
    "                    # Update online gallery\n",
    "                    online_gallery[trk.stable_name].append(trk.embedding.copy())\n",
    "                    \n",
    "                    # Segment\n",
    "                    try:\n",
    "                        image_pil = Image.fromarray(output_frame)\n",
    "                        body_bbox = face_to_body_bbox(trk.bbox, (height, width))\n",
    "                        mask = segment_by_box(image_pil, body_bbox.tolist())\n",
    "                        mask = clean_mask(mask, kernel_size=5)\n",
    "                        smoothed_mask = mask_smoother.update(trk.id, mask)\n",
    "                        \n",
    "                        output_frame = create_overlay(\n",
    "                            output_frame, smoothed_mask, \n",
    "                            list(trk.color), alpha=0.5\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Segmentation error: {e}\")\n",
    "                    \n",
    "                    # Draw bbox with label background\n",
    "                    x1, y1, x2, y2 = trk.bbox.astype(int)\n",
    "                    cv2.rectangle(output_frame, (x1, y1), (x2, y2), trk.color, 2)\n",
    "                    \n",
    "                    label = f\"{trk.stable_name}#{trk.id} ({trk.stable_sim:.2f})\"\n",
    "                    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(output_frame, (x1, y1-th-8), (x1+tw+4, y1), trk.color, -1)\n",
    "                    cv2.putText(output_frame, label, (x1+2, y1-4),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "                \n",
    "                # Write\n",
    "                out.write(cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR))\n",
    "                processed_count += 1\n",
    "                \n",
    "                # Debug ‡∏ó‡∏∏‡∏Å 30 frames\n",
    "                if frame_idx % 30 == 0 and len(tracks) > 0:\n",
    "                    active = [f\"{t.stable_name}#{t.id}\" for t in tracks if t.is_confirmed]\n",
    "                    print(f\"[Frame {frame_idx}] Active tracks: {active}\")\n",
    "            \n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {processed_count} frames\")\n",
    "    print(f\"   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved functions loaded! ‡πÉ‡∏ä‡πâ process_video_improved() ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cell ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô Jupyter Notebook\n",
    "‡∏ß‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ô cell ‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED VIDEO PROCESSING (‡∏ß‡∏≤‡∏á‡πÉ‡∏ô Cell ‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á main.ipynb)\n",
    "# =============================================================================\n",
    "\n",
    "from collections import deque, defaultdict, Counter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    xA, yA = max(boxA[0], boxB[0]), max(boxA[1], boxB[1])\n",
    "    xB, yB = min(boxA[2], boxB[2]), min(boxA[3], boxB[3])\n",
    "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "    areaA = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
    "    areaB = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
    "    return inter / (areaA + areaB - inter + 1e-6)\n",
    "\n",
    "def best_match_hungarian(face_embeddings, embeddings_db, threshold=0.45):\n",
    "    if len(face_embeddings) == 0 or len(embeddings_db) == 0:\n",
    "        return []\n",
    "    member_names = list(embeddings_db.keys())\n",
    "    num_faces = len(face_embeddings)\n",
    "    num_members = len(member_names)\n",
    "    similarity_matrix = np.zeros((num_faces, num_members))\n",
    "    for i, (idx, embedding, det_score, bbox) in enumerate(face_embeddings):\n",
    "        for j, name in enumerate(member_names):\n",
    "            ref_emb = embeddings_db[name]\n",
    "            if isinstance(ref_emb, np.ndarray) and ref_emb.ndim == 2:\n",
    "                sim = float(np.max(ref_emb @ embedding))\n",
    "            else:\n",
    "                sim = cosine_sim(embedding, ref_emb)\n",
    "            similarity_matrix[i, j] = sim\n",
    "    cost_matrix = -similarity_matrix\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    matches = [None] * num_faces\n",
    "    for face_idx, member_idx in zip(row_ind, col_ind):\n",
    "        similarity = similarity_matrix[face_idx, member_idx]\n",
    "        if similarity >= threshold:\n",
    "            member_name = member_names[member_idx]\n",
    "            bbox = face_embeddings[face_idx][3]\n",
    "            matches[face_idx] = (face_idx, member_name, similarity, bbox)\n",
    "    return matches\n",
    "\n",
    "class ImprovedTrack:\n",
    "    def __init__(self, track_id, bbox, embedding, name, sim, bbox_momentum=0.65):\n",
    "        self.id = track_id\n",
    "        self.bbox = np.array(bbox, dtype=float)\n",
    "        self.embedding = normalize_embedding(embedding)\n",
    "        self.name_history = deque([name], maxlen=15)\n",
    "        self.sim_history = deque([sim], maxlen=15)\n",
    "        self.lost = 0\n",
    "        self.age = 1\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "        np.random.seed(track_id + 42)\n",
    "        self.color = tuple(int(x) for x in np.random.randint(50, 230, size=3))\n",
    "    def update(self, bbox, embedding, name, sim):\n",
    "        self.bbox = self.bbox_momentum * np.array(bbox) + (1 - self.bbox_momentum) * self.bbox\n",
    "        self.embedding = normalize_embedding(0.7 * self.embedding + 0.3 * normalize_embedding(embedding))\n",
    "        self.name_history.append(name)\n",
    "        self.sim_history.append(sim)\n",
    "        self.lost = 0\n",
    "        self.age += 1\n",
    "    def mark_lost(self):\n",
    "        self.lost += 1\n",
    "        return self.lost\n",
    "    @property\n",
    "    def stable_name(self):\n",
    "        if len(self.name_history) < 3:\n",
    "            valid_names = [n for n in self.name_history if n != \"Unknown\"]\n",
    "            return valid_names[-1] if valid_names else \"Unknown\"\n",
    "        counts = Counter(self.name_history)\n",
    "        if not counts:\n",
    "            return \"Unknown\"\n",
    "        most_common = counts.most_common(1)[0]\n",
    "        name, count = most_common\n",
    "        if name != \"Unknown\" and count / len(self.name_history) >= 0.4:\n",
    "            return name\n",
    "        return \"Unknown\"\n",
    "    @property\n",
    "    def stable_sim(self):\n",
    "        if not self.sim_history:\n",
    "            return 0.0\n",
    "        return float(np.mean(self.sim_history))\n",
    "    @property\n",
    "    def is_confirmed(self):\n",
    "        return self.age >= 3 and self.stable_name != \"Unknown\"\n",
    "\n",
    "class ImprovedTracker:\n",
    "    def __init__(self, iou_thr=0.35, embed_thr=0.40, max_lost=8, sim_update_thr=0.40, bbox_momentum=0.65):\n",
    "        self.iou_thr = iou_thr\n",
    "        self.embed_thr = embed_thr\n",
    "        self.max_lost = max_lost\n",
    "        self.sim_update_thr = sim_update_thr\n",
    "        self.bbox_momentum = bbox_momentum\n",
    "        self.tracks = []\n",
    "        self.next_id = 0\n",
    "    def compute_similarity_matrix(self, tracks, detections):\n",
    "        n_tracks, n_dets = len(tracks), len(detections)\n",
    "        if n_tracks == 0 or n_dets == 0:\n",
    "            return np.zeros((n_tracks, n_dets))\n",
    "        cost_matrix = np.zeros((n_tracks, n_dets))\n",
    "        for i, trk in enumerate(tracks):\n",
    "            for j, det in enumerate(detections):\n",
    "                iou_score = iou(trk.bbox, det['bbox'])\n",
    "                embed_sim = cosine_sim(trk.embedding, det['embedding'])\n",
    "                cost_matrix[i, j] = 0.4 * iou_score + 0.6 * embed_sim\n",
    "        return cost_matrix\n",
    "    def step(self, detections, frame_idx=0):\n",
    "        valid_detections = [d for d in detections if d['sim'] >= self.sim_update_thr]\n",
    "        if len(self.tracks) == 0:\n",
    "            for det in valid_detections:\n",
    "                self._create_track(det)\n",
    "        else:\n",
    "            cost_matrix = self.compute_similarity_matrix(self.tracks, valid_detections)\n",
    "            n_tracks, n_dets = len(self.tracks), len(valid_detections)\n",
    "            if n_tracks > 0 and n_dets > 0:\n",
    "                row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n",
    "                matched_tracks, matched_dets = set(), set()\n",
    "                for i, j in zip(row_ind, col_ind):\n",
    "                    iou_score = iou(self.tracks[i].bbox, valid_detections[j]['bbox'])\n",
    "                    embed_sim = cosine_sim(self.tracks[i].embedding, valid_detections[j]['embedding'])\n",
    "                    if iou_score >= self.iou_thr and embed_sim >= self.embed_thr:\n",
    "                        self.tracks[i].update(valid_detections[j]['bbox'], valid_detections[j]['embedding'],\n",
    "                                              valid_detections[j]['name'], valid_detections[j]['sim'])\n",
    "                        matched_tracks.add(i)\n",
    "                        matched_dets.add(j)\n",
    "                for i in range(n_tracks):\n",
    "                    if i not in matched_tracks:\n",
    "                        self.tracks[i].mark_lost()\n",
    "                for j in range(n_dets):\n",
    "                    if j not in matched_dets:\n",
    "                        self._create_track(valid_detections[j])\n",
    "            else:\n",
    "                for trk in self.tracks:\n",
    "                    trk.mark_lost()\n",
    "                for det in valid_detections:\n",
    "                    self._create_track(det)\n",
    "        self.tracks = [t for t in self.tracks if t.lost <= self.max_lost]\n",
    "        return self.tracks\n",
    "    def _create_track(self, detection):\n",
    "        trk = ImprovedTrack(self.next_id, detection['bbox'], detection['embedding'],\n",
    "                            detection['name'], detection['sim'], bbox_momentum=self.bbox_momentum)\n",
    "        self.tracks.append(trk)\n",
    "        self.next_id += 1\n",
    "\n",
    "class TemporalMaskSmoother:\n",
    "    def __init__(self, window_size=5):\n",
    "        self.window_size = window_size\n",
    "        self.track_masks = defaultdict(lambda: deque(maxlen=window_size))\n",
    "    def update(self, track_id, mask):\n",
    "        self.track_masks[track_id].append(mask.astype(np.float32))\n",
    "        if len(self.track_masks[track_id]) >= 3:\n",
    "            smoothed = np.mean(self.track_masks[track_id], axis=0)\n",
    "            return (smoothed > 0.5).astype(np.uint8)\n",
    "        return mask\n",
    "\n",
    "def process_video_improved(\n",
    "    input_path, output_path, target_member=None, frame_sampling=1,\n",
    "    det_thresh=0.45, rec_thresh=0.45, tracker_iou=0.35, tracker_embed=0.40,\n",
    "    max_lost=8, bbox_momentum=0.65, segment_mode=\"target\"):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video: {input_path}\")\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"üé¨ Video: {width}x{height} @ {fps:.1f}fps, {total_frames} frames\")\n",
    "    print(f\"üéØ Target: {target_member if target_member else 'All members'}\")\n",
    "    print(f\"üîß Recognition threshold: {rec_thresh}\")\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps / frame_sampling, (width, height))\n",
    "    tracker = ImprovedTracker(iou_thr=tracker_iou, embed_thr=tracker_embed, max_lost=max_lost,\n",
    "                              sim_update_thr=rec_thresh * 0.9, bbox_momentum=bbox_momentum)\n",
    "    online_gallery = defaultdict(lambda: deque(maxlen=30))\n",
    "    mask_smoother = TemporalMaskSmoother(window_size=5)\n",
    "    frame_idx, processed_count = 0, 0\n",
    "    from tqdm import tqdm\n",
    "    with tqdm(total=total_frames, desc=\"Processing\") as pbar:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if frame_idx % frame_sampling == 0:\n",
    "                faces = face_analyzer.get(frame)\n",
    "                faces = [f for f in faces if f.det_score >= det_thresh]\n",
    "                face_embeddings = []\n",
    "                for i, face in enumerate(faces):\n",
    "                    bbox = face.bbox.astype(float)\n",
    "                    emb = normalize_embedding(face.normed_embedding)\n",
    "                    face_embeddings.append((i, emb, face.det_score, bbox))\n",
    "                matches = best_match_hungarian(face_embeddings, embeddings_db, threshold=rec_thresh)\n",
    "                detections = []\n",
    "                for match in matches:\n",
    "                    if match is not None:\n",
    "                        face_idx, name, sim, bbox = match\n",
    "                        detections.append({'bbox': bbox, 'embedding': face_embeddings[face_idx][1],\n",
    "                                           'name': name, 'sim': sim})\n",
    "                tracks = tracker.step(detections, frame_idx)\n",
    "                output_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                for trk in tracks:\n",
    "                    if not trk.is_confirmed:\n",
    "                        continue\n",
    "                    if target_member and trk.stable_name != target_member:\n",
    "                        continue\n",
    "                    online_gallery[trk.stable_name].append(trk.embedding.copy())\n",
    "                    should_seg = (segment_mode == \"all\") or (segment_mode == \"target\" and trk.stable_name == target_member)\n",
    "                    if should_seg:\n",
    "                        try:\n",
    "                            image_pil = Image.fromarray(output_frame)\n",
    "                            body_bbox = face_to_body_bbox(trk.bbox, (height, width))\n",
    "                            mask = segment_by_box(image_pil, body_bbox.tolist())\n",
    "                            mask = clean_mask(mask, kernel_size=5)\n",
    "                            smoothed_mask = mask_smoother.update(trk.id, mask)\n",
    "                            output_frame = create_overlay(output_frame, smoothed_mask, list(trk.color), alpha=0.5)\n",
    "                        except Exception as e:\n",
    "                            print(f\"[WARN] Segmentation error: {e}\")\n",
    "                    x1, y1, x2, y2 = trk.bbox.astype(int)\n",
    "                    cv2.rectangle(output_frame, (x1, y1), (x2, y2), trk.color, 2)\n",
    "                    label = f\"{trk.stable_name}#{trk.id} ({trk.stable_sim:.2f})\"\n",
    "                    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                    cv2.rectangle(output_frame, (x1, y1-th-8), (x1+tw+4, y1), trk.color, -1)\n",
    "                    cv2.putText(output_frame, label, (x1+2, y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
    "                out.write(cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR))\n",
    "                processed_count += 1\n",
    "                if frame_idx % 30 == 0 and len(tracks) > 0:\n",
    "                    active = [f\"{t.stable_name}#{t.id}\" for t in tracks if t.is_confirmed]\n",
    "                    print(f\"[Frame {frame_idx}] Active tracks: {active}\")\n",
    "            frame_idx += 1\n",
    "            pbar.update(1)\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {processed_count} frames\")\n",
    "    print(f\"   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    return output_path\n",
    "\n",
    "print(\"‚úÖ Improved functions loaded! ‡πÉ‡∏ä‡πâ process_video_improved() ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Video: 1920x1080 @ 30.0fps, 909 frames\n",
      "üéØ Target: Wonyoung\n",
      "üîß Recognition threshold: 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/909 [00:00<?, ?it/s]/root/SEGMENTATION_IVE/.venv/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n",
      "Processing:   0%|          | 2/909 [00:00<00:58, 15.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 0] Active tracks: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   4%|‚ñé         | 32/909 [00:02<01:55,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 30] Active tracks: ['Gaeul#0', 'Leeseo#1', 'Wonyoung#3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   7%|‚ñã         | 62/909 [00:06<01:55,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 60] Active tracks: ['Gaeul#0', 'Leeseo#1', 'Wonyoung#3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  10%|‚ñà         | 92/909 [00:11<02:08,  6.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 90] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Gaeul#7', 'Yujin#8', 'Wonyoung#9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  13%|‚ñà‚ñé        | 122/909 [00:16<02:06,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 120] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Wonyoung#9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|‚ñà‚ñã        | 152/909 [00:21<02:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 150] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Wonyoung#9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  20%|‚ñà‚ñà        | 182/909 [00:26<01:58,  6.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 180] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Wonyoung#9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|‚ñà‚ñà‚ñé       | 213/909 [00:28<00:49, 13.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 210] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  27%|‚ñà‚ñà‚ñã       | 242/909 [00:33<01:48,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 240] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|‚ñà‚ñà‚ñâ       | 272/909 [00:37<01:41,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 270] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|‚ñà‚ñà‚ñà‚ñé      | 302/909 [00:42<01:36,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 300] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  37%|‚ñà‚ñà‚ñà‚ñã      | 332/909 [00:47<01:32,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 330] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  40%|‚ñà‚ñà‚ñà‚ñâ      | 362/909 [00:52<01:26,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 360] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 392/909 [00:57<01:23,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 390] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 423/909 [01:01<00:51,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 420] Active tracks: ['Rei#4', 'Liz#5', 'Leeseo#6', 'Yujin#8', 'Gaeul#10', 'Wonyoung#11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 512/909 [01:07<00:56,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 510] Active tracks: ['Liz#12', 'Rei#13', 'Leeseo#15', 'Wonyoung#16', 'Yujin#17']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 543/909 [01:09<00:25, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 540] Active tracks: ['Liz#12', 'Rei#13', 'Leeseo#15', 'Yujin#17']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 572/909 [01:13<00:45,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 570] Active tracks: ['Gaeul#19', 'Leeseo#20', 'Wonyoung#21']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 602/909 [01:17<00:41,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 600] Active tracks: ['Gaeul#19', 'Leeseo#20', 'Wonyoung#21']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 632/909 [01:22<00:44,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 630] Active tracks: ['Leeseo#22', 'Gaeul#23', 'Rei#24', 'Liz#25', 'Yujin#26', 'Wonyoung#27']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 662/909 [01:27<00:40,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 660] Active tracks: ['Leeseo#22', 'Gaeul#23', 'Rei#24', 'Liz#25', 'Yujin#26', 'Wonyoung#27']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 692/909 [01:32<00:35,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 690] Active tracks: ['Leeseo#22', 'Gaeul#23', 'Rei#24', 'Liz#25', 'Yujin#26', 'Wonyoung#27']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 722/909 [01:37<00:30,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 720] Active tracks: ['Leeseo#22', 'Gaeul#23', 'Rei#24', 'Liz#25', 'Yujin#26', 'Wonyoung#27']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 812/909 [01:41<00:05, 18.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 810] Active tracks: ['Gaeul#28', 'Leeseo#29', 'Liz#30', 'Rei#31', 'Yujin#32', 'Wonyoung#33']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 842/909 [01:46<00:11,  6.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 840] Active tracks: ['Gaeul#28', 'Leeseo#29', 'Liz#30', 'Rei#31', 'Yujin#32', 'Wonyoung#33']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 872/909 [01:50<00:06,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 870] Active tracks: ['Gaeul#28', 'Leeseo#29', 'Liz#30', 'Yujin#32', 'Wonyoung#33']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 902/909 [01:55<00:01,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Frame 900] Active tracks: ['Gaeul#28', 'Leeseo#29', 'Liz#30', 'Yujin#32', 'Wonyoung#33', 'Rei#35']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 909/909 [01:56<00:00,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• 909 frames\n",
      "   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: outputs/segmented_wonyoung_improved.mp4\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'outputs/segmented_wonyoung_improved.mp4'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SEGMENTATION_IVE/.venv/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ‡∏û‡∏ö 6 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (det_score >= 0.45)\n",
      "   [OK] Gaeul: sim=0.6127, det=0.893\n",
      "   [OK] Rei: sim=0.5187, det=0.885\n",
      "   [OK] Yujin: sim=0.6419, det=0.884\n",
      "   [OK] Wonyoung: sim=0.6140, det=0.871\n",
      "   [OK] Leeseo: sim=0.6512, det=0.850\n",
      "   [OK] Liz: sim=0.6115, det=0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/SEGMENTATION_IVE/.venv/lib/python3.10/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ‡∏û‡∏ö 6 ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (det_score >= 0.45)\n",
      "   [OK] Gaeul: sim=0.6127, det=0.893\n",
      "   [OK] Rei: sim=0.5187, det=0.885\n",
      "   [OK] Yujin: sim=0.6419, det=0.884\n",
      "   [OK] Wonyoung: sim=0.6140, det=0.871\n",
      "   [OK] Leeseo: sim=0.6512, det=0.850\n",
      "   [OK] Liz: sim=0.6115, det=0.819\n"
     ]
    }
   ],
   "source": [
    "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô IMPROVED (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠)\n",
    "input_video = \"Input/IVE-30s.mp4\"\n",
    "output_video = \"outputs/segmented_wonyoung_improved.mp4\"\n",
    "\n",
    "# ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 1: Segment ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Wonyoung (‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)\n",
    "process_video_improved(\n",
    "    input_video, \n",
    "    output_video, \n",
    "    target_member=\"Wonyoung\",\n",
    "    frame_sampling=1,           # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏∏‡∏Å‡πÄ‡∏ü‡∏£‡∏°\n",
    "    rec_thresh=0.45,            # ‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å 0.40 ‡∏•‡∏î false positive\n",
    "    segment_mode=\"target\"       # segment ‡πÅ‡∏Ñ‡πà target\n",
    ")\n",
    "\n",
    "# ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 2: ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà segment\n",
    "# process_video_improved(\n",
    "#     input_video, \n",
    "#     \"outputs/all_members.mp4\",\n",
    "#     target_member=None,         # None = ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô\n",
    "#     frame_sampling=1,\n",
    "#     rec_thresh=0.45,\n",
    "#     segment_mode=\"none\"         # ‡πÑ‡∏°‡πà segment\n",
    "# )\n",
    "\n",
    "# ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà 3: Segment ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô (‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤)\n",
    "# process_video_improved(\n",
    "#     input_video, \n",
    "#     \"outputs/all_segmented.mp4\",\n",
    "#     target_member=None,\n",
    "#     frame_sampling=2,           # ‡∏ó‡∏∏‡∏Å 2 ‡πÄ‡∏ü‡∏£‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß\n",
    "#     rec_thresh=0.45,\n",
    "#     segment_mode=\"all\"          # segment ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéâ ‡∏™‡∏£‡∏∏‡∏õ\n",
    "\n",
    "‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
    "1. ‚úÖ Environment Setup - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
    "2. ‚úÖ Face Embedding Database - ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IVE\n",
    "3. ‚úÖ Identity Matching - ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Hungarian Algorithm\n",
    "4. ‚úÖ SAM 3 Engine - Segmentation ‡∏î‡πâ‡∏ß‡∏¢ box prompt\n",
    "5. ‚úÖ Integration Pipeline - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
    "6. ‚úÖ Gradio UI - Web interface ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
    "7. ‚úÖ Video Inference (IMPROVED) - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏° tracking ‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô\n",
    "\n",
    "### üîß ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏ô Section 7 (Video Inference)\n",
    "\n",
    "**‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:** ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û `image.png` ‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ Wonyoung 2 ‡∏Ñ‡∏ô (Wonyoung#8 ‡πÅ‡∏•‡∏∞ Wonyoung#3)\n",
    "\n",
    "**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:**\n",
    "- Face recognition ‡πÑ‡∏°‡πà‡∏°‡∏µ 1-to-1 matching constraint ‚Üí ‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ match ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
    "- Recognition threshold ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (0.40) ‚Üí false positive ‡∏™‡∏π‡∏á\n",
    "\n",
    "**‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**\n",
    "| ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå | ‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏° | ‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡∏°‡πà | ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå |\n",
    "|---------|--------|--------|---------|\n",
    "| Matching Algorithm | Greedy | **Hungarian Algorithm** | 1 ‡∏Ñ‡∏ô = 1 track |\n",
    "| Recognition Threshold | 0.40 | **0.45** | ‡∏•‡∏î false positive |\n",
    "| IoU Threshold | 0.50 | **0.35** | ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô |\n",
    "| Max Lost Frames | 5 | **8** | Track ‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô |\n",
    "| Temporal Voting | ‚ùå | **Majority voting (40%)** | ‡∏ä‡∏∑‡πà‡∏≠‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô |\n",
    "| Mask Smoothing | Global | **Per-track** | Segmentation ‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô |\n",
    "\n",
    "**‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô RTX 6000 (48GB VRAM)!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
