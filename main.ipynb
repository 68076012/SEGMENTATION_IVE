{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé≠ Identity-Aware Segmentation with SAM 3 & InsightFace\n",
        "\n",
        "**‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:** ‡∏£‡∏∞‡∏ö‡∏ö Segmentation ‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ß‡∏á IVE\n",
        "\n",
        "**‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ:**\n",
        "- SAM 3 (Segment Anything Model 3) - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Segmentation\n",
        "- InsightFace - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Face Recognition\n",
        "- RTX 6000 (48GB VRAM) Optimized\n",
        "\n",
        "**‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE 6 ‡∏Ñ‡∏ô:** Wonyoung, Yujin, Gaeul, Liz, Leeseo, Rei"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. [Environment Setup](#section-1)\n",
        "2. [Face Embedding Database](#section-2)\n",
        "3. [Identity Matching](#section-3)\n",
        "4. [SAM 3 Engine](#section-4)\n",
        "5. [Integration Pipeline](#section-5)\n",
        "6. [Gradio UI](#section-6)\n",
        "7. [Video Inference](#section-7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-1'></a>\n",
        "## Section 1: Environment Setup üîß\n",
        "\n",
        "‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RTX 6000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CELL 1.1: Install Dependencies\n",
        "# =============================================================================\n",
        "\n",
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch 2.7.0 ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö CUDA 12.1\n",
        "!pip install -q torch==2.7.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Face Recognition ‡πÅ‡∏•‡∏∞ Segmentation libraries\n",
        "!pip install -q insightface onnxruntime-gpu opencv-python Pillow\n",
        "\n",
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á UI ‡πÅ‡∏•‡∏∞ Utilities\n",
        "!pip install -q gradio matplotlib scikit-learn scipy\n",
        "!pip install -q huggingface-hub transformers accelerate\n",
        "\n",
        "# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Video Processing\n",
        "!pip install -q tqdm imageio imageio-ffmpeg av\n",
        "\n",
        "print(\"‚úÖ Dependencies ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CELL 1.2: Clone ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SAM 3\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå sam3 ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "if not os.path.exists(\"sam3\"):\n",
        "    print(\"üì• ‡∏Å‡∏≥‡∏•‡∏±‡∏á clone SAM 3 repository...\")\n",
        "    !git clone https://github.com/facebookresearch/sam3.git\n",
        "    print(\"‚úÖ Clone SAM 3 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")\n",
        "else:\n",
        "    print(\"üìÅ SAM 3 directory ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß\")\n",
        "\n",
        "# ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô directory ‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á\n",
        "%cd sam3\n",
        "!pip install -q -e \".[notebooks]\"\n",
        "%cd ..\n",
        "\n",
        "print(\"‚úÖ ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á SAM 3 ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CELL 1.3: HuggingFace Login\n",
        "# =============================================================================\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "print(\"üîë ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà HuggingFace Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\")\n",
        "print(\"   - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà https://huggingface.co/settings/tokens\")\n",
        "print(\"   - ‡∏™‡∏£‡πâ‡∏≤‡∏á token ‡πÉ‡∏´‡∏°‡πà (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ access ‡∏ñ‡∏∂‡∏á SAM 3)\")\n",
        "print(\"   - ‡∏ß‡∏≤‡∏á token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:\")\n",
        "login()\n",
        "\n",
        "print(\"‚úÖ Login HuggingFace ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CELL 1.4: Verify GPU ‡πÅ‡∏•‡∏∞ CUDA\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üñ•Ô∏è  GPU Information\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
        "    \n",
        "    cuda_version = torch.version.cuda\n",
        "    print(f\"üìå CUDA Version: {cuda_version}\")\n",
        "    \n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "    print(f\"üìå Total VRAM: {total_memory:.2f} GB\")\n",
        "    \n",
        "    major = torch.cuda.get_device_capability(0)[0]\n",
        "    minor = torch.cuda.get_device_capability(0)[1]\n",
        "    print(f\"üìå Compute Capability: {major}.{minor}\")\n",
        "    \n",
        "    if major >= 8:\n",
        "        print(\"‚úÖ bfloat16 supported!\")\n",
        "else:\n",
        "    print(\"‚ùå CUDA not available!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# CELL 1.5: Import All Libraries\n",
        "# =============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Face Recognition\n",
        "from insightface.app import FaceAnalysis\n",
        "\n",
        "# SAM 3\n",
        "from sam3 import SAM3ImagePredictor, build_sam3\n",
        "\n",
        "# UI\n",
        "import gradio as gr\n",
        "\n",
        "# Utilities\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-2'></a>\n",
        "## Section 2: Face Embedding Database üë§\n",
        "\n",
        "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Face Embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE 6 ‡∏Ñ‡∏ô"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 2: Face Embedding Database Creation\n",
        "# =============================================================================\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Mapping ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\n",
        "MEMBER_MAPPING = {\n",
        "    \"An_Yujin\": \"Yujin\",\n",
        "    \"Jang_Wonyoung\": \"Wonyoung\",\n",
        "    \"Kim_Gaeul\": \"Gaeul\",\n",
        "    \"Kim_Jiwon\": \"Liz\",\n",
        "    \"Lee_Hyunseo\": \"Leeseo\",\n",
        "    \"Naoi_Rei\": \"Rei\"\n",
        "}\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î InsightFace\n",
        "print(\"üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î InsightFace model (buffalo_l)...\")\n",
        "face_analyzer = FaceAnalysis(\n",
        "    name='buffalo_l',\n",
        "    root='./insightface_models',\n",
        "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        ")\n",
        "face_analyzer.prepare(ctx_id=0, det_size=(640, 640))\n",
        "print(\"‚úÖ InsightFace model ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!\")\n",
        "\n",
        "def normalize_embedding(embedding):\n",
        "    \"\"\"‡∏ó‡∏≥ L2 Normalization ‡∏ö‡∏ô face embedding\"\"\"\n",
        "    norm = np.linalg.norm(embedding)\n",
        "    if norm < 1e-10:\n",
        "        return embedding\n",
        "    return embedding / norm\n",
        "\n",
        "def create_embedding_database(dataset_path, face_analyzer, member_mapping):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• face embeddings\"\"\"\n",
        "    embeddings_db = {}\n",
        "    \n",
        "    for folder_name, member_name in member_mapping.items():\n",
        "        member_path = Path(dataset_path) / folder_name\n",
        "        \n",
        "        if not member_path.exists():\n",
        "            print(f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {member_path}\")\n",
        "            continue\n",
        "        \n",
        "        image_files = list(member_path.glob('*.jpg')) + list(member_path.glob('*.png'))\n",
        "        member_embeddings = []\n",
        "        \n",
        "        for img_path in image_files:\n",
        "            img_bgr = cv2.imread(str(img_path))\n",
        "            if img_bgr is None:\n",
        "                continue\n",
        "            \n",
        "            faces = face_analyzer.get(img_bgr)\n",
        "            if len(faces) > 0:\n",
        "                face = max(faces, key=lambda f: (f.bbox[2]-f.bbox[0])*(f.bbox[3]-f.bbox[1]))\n",
        "                normalized_embedding = normalize_embedding(face.embedding)\n",
        "                member_embeddings.append(normalized_embedding)\n",
        "        \n",
        "        if len(member_embeddings) > 0:\n",
        "            avg_embedding = np.mean(member_embeddings, axis=0)\n",
        "            avg_embedding = normalize_embedding(avg_embedding)\n",
        "            embeddings_db[member_name] = avg_embedding\n",
        "            print(f\"‚úÖ {member_name}: {len(member_embeddings)} faces extracted\")\n",
        "    \n",
        "    return embeddings_db\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings database\n",
        "embeddings_db = create_embedding_database(\"Dataset\", face_analyzer, MEMBER_MAPPING)\n",
        "print(f\"\\n‚úÖ Face Embedding Database ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô! ({len(embeddings_db)} ‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å)\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-3'></a>\n",
        "## Section 3: Identity Matching üéØ\n",
        "\n",
        "‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE ‡∏à‡∏≤‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û\n",
        "‡πÉ‡∏ä‡πâ Cosine Similarity ‡πÅ‡∏•‡∏∞ Hungarian Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 3: Identity Matching Functions\n",
        "# =============================================================================\n",
        "\n",
        "def cosine_similarity(embedding1, embedding2):\n",
        "    \"\"\"‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 2 embeddings\"\"\"\n",
        "    return np.dot(embedding1, embedding2)\n",
        "\n",
        "def hungarian_matching(faces, embeddings_db, threshold=0.45):\n",
        "    \"\"\"\n",
        "    ‡πÉ‡∏ä‡πâ Hungarian Algorithm ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å‡∏ó‡∏µ‡πà optimal\n",
        "    ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô\n",
        "    \"\"\"\n",
        "    if len(faces) == 0 or len(embeddings_db) == 0:\n",
        "        return []\n",
        "    \n",
        "    member_names = list(embeddings_db.keys())\n",
        "    member_embeddings = [embeddings_db[name] for name in member_names]\n",
        "    \n",
        "    num_faces = len(faces)\n",
        "    num_members = len(member_names)\n",
        "    \n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Similarity Matrix\n",
        "    similarity_matrix = np.zeros((num_faces, num_members))\n",
        "    \n",
        "    for i, face in enumerate(faces):\n",
        "        face_embedding = face.embedding / (np.linalg.norm(face.embedding) + 1e-10)\n",
        "        for j, member_emb in enumerate(member_embeddings):\n",
        "            similarity = cosine_similarity(face_embedding, member_emb)\n",
        "            similarity_matrix[i, j] = similarity\n",
        "    \n",
        "    # ‡πÉ‡∏ä‡πâ Hungarian Algorithm\n",
        "    cost_matrix = -similarity_matrix\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "    \n",
        "    matches = [None] * num_faces\n",
        "    \n",
        "    for face_idx, member_idx in zip(row_ind, col_ind):\n",
        "        similarity = similarity_matrix[face_idx, member_idx]\n",
        "        if similarity >= threshold:\n",
        "            member_name = member_names[member_idx]\n",
        "            matches[face_idx] = (face_idx, member_name, similarity)\n",
        "    \n",
        "    return matches\n",
        "\n",
        "def identify_all_members(image_bgr, face_analyzer, embeddings_db, threshold=0.45):\n",
        "    \"\"\"‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å IVE ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏†‡∏≤‡∏û\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    faces = face_analyzer.get(image_bgr)\n",
        "    \n",
        "    if len(faces) == 0:\n",
        "        print(\"‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û\")\n",
        "        return results\n",
        "    \n",
        "    print(f\"üîç ‡∏û‡∏ö {len(faces)} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û\")\n",
        "    \n",
        "    matches = hungarian_matching(faces, embeddings_db, threshold)\n",
        "    \n",
        "    for i, face in enumerate(faces):\n",
        "        bbox = face.bbox.astype(int).tolist()\n",
        "        \n",
        "        if matches[i] is not None:\n",
        "            _, member_name, similarity = matches[i]\n",
        "            results.append({\n",
        "                'name': member_name,\n",
        "                'bbox': bbox,\n",
        "                'similarity': float(similarity)\n",
        "            })\n",
        "            print(f\"   ‚úÖ {member_name}: similarity={similarity:.4f}\")\n",
        "        else:\n",
        "            results.append({\n",
        "                'name': 'Unknown',\n",
        "                'bbox': bbox,\n",
        "                'similarity': None\n",
        "            })\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Identity matching functions loaded!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-4'></a>\n",
        "## Section 4: SAM 3 Engine ‚úÇÔ∏è\n",
        "\n",
        "‡πÇ‡∏´‡∏•‡∏î SAM 3 ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 4: SAM 3 Engine\n",
        "# =============================================================================\n",
        "\n",
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ precision ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RTX 6000\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üîß SAM 3 Engine: Using device = {DEVICE}\")\n",
        "\n",
        "# ‡πÇ‡∏´‡∏•‡∏î SAM 3 Model\n",
        "print(\"\\nüì¶ Loading SAM 3 Model...\")\n",
        "\n",
        "# ‡πÉ‡∏ä‡πâ SAM 3 Large ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î\n",
        "sam3_checkpoint = \"facebook/sam3-hiera-large\"\n",
        "\n",
        "from transformers import Sam3Model, Sam3Processor\n",
        "\n",
        "sam3_processor = Sam3Processor.from_pretrained(sam3_checkpoint)\n",
        "sam3_model = Sam3Model.from_pretrained(sam3_checkpoint).to(DEVICE)\n",
        "\n",
        "# Optimize ‡∏î‡πâ‡∏ß‡∏¢ bfloat16\n",
        "sam3_model = sam3_model.to(torch.bfloat16)\n",
        "\n",
        "# ‡πÉ‡∏ä‡πâ torch.compile ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RTX 6000\n",
        "sam3_model = torch.compile(sam3_model, mode=\"reduce-overhead\")\n",
        "sam3_model.eval()\n",
        "\n",
        "print(\"‚úÖ SAM 3 Model loaded successfully!\")\n",
        "\n",
        "def segment_by_box(image_pil, box_xyxy):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á mask ‡∏à‡∏≤‡∏Å bounding box\"\"\"\n",
        "    inputs = sam3_processor(\n",
        "        images=image_pil,\n",
        "        input_boxes=[[[box_xyxy]]],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
        "            outputs = sam3_model(**inputs)\n",
        "    \n",
        "    masks = sam3_processor.image_processor.post_process_masks(\n",
        "        outputs.pred_masks,\n",
        "        inputs[\"original_sizes\"],\n",
        "        inputs[\"reshaped_input_sizes\"]\n",
        "    )\n",
        "    \n",
        "    mask_tensor = masks[0]\n",
        "    iou_scores = outputs.iou_scores[0, 0]\n",
        "    best_idx = torch.argmax(iou_scores).item()\n",
        "    best_mask = mask_tensor[0, best_idx].cpu().numpy()\n",
        "    \n",
        "    return (best_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "print(\"‚úÖ SAM 3 segmentation function ready!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-5'></a>\n",
        "## Section 5: Integration Pipeline üîó\n",
        "\n",
        "‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á InsightFace ‡∏Å‡∏±‡∏ö SAM 3 - The Magic Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 5: Integration Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "def face_to_body_bbox(face_bbox, img_shape, width_scale=3.6, height_top_scale=0.5, height_bottom_scale=5.5):\n",
        "    \"\"\"‡∏Ç‡∏¢‡∏≤‡∏¢ face bbox ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏ï‡∏±‡∏ß\"\"\"\n",
        "    x1, y1, x2, y2 = face_bbox.astype(float)\n",
        "    face_center_x = (x1 + x2) / 2.0\n",
        "    face_center_y = (y1 + y2) / 2.0\n",
        "    face_width = x2 - x1\n",
        "    face_height = y2 - y1\n",
        "    \n",
        "    img_h, img_w = img_shape\n",
        "    \n",
        "    half_body_width = (face_width * width_scale) / 2.0\n",
        "    body_x1 = face_center_x - half_body_width\n",
        "    body_x2 = face_center_x + half_body_width\n",
        "    \n",
        "    body_y1 = face_center_y - (face_height * height_top_scale)\n",
        "    body_y2 = face_center_y + (face_height * height_bottom_scale)\n",
        "    \n",
        "    body_x1 = max(0, body_x1)\n",
        "    body_y1 = max(0, body_y1)\n",
        "    body_x2 = min(img_w - 1, body_x2)\n",
        "    body_y2 = min(img_h - 1, body_y2)\n",
        "    \n",
        "    return np.array([body_x1, body_y1, body_x2, body_y2], dtype=np.int32)\n",
        "\n",
        "def create_overlay(image_rgb, mask, color=[0, 255, 128], alpha=0.5):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û overlay ‡πÇ‡∏î‡∏¢‡πÉ‡∏™‡πà‡∏™‡∏µ‡∏ó‡∏±‡∏ö mask\"\"\"\n",
        "    overlay = image_rgb.copy()\n",
        "    mask_bool = mask.astype(bool)\n",
        "    color_layer = np.zeros_like(image_rgb)\n",
        "    color_layer[mask_bool] = color\n",
        "    overlay = cv2.addWeighted(overlay, 1.0, color_layer, alpha, 0)\n",
        "    return overlay\n",
        "\n",
        "def create_cutout(image_rgb, mask):\n",
        "    \"\"\"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û cutout ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™\"\"\"\n",
        "    mask_bool = mask.astype(bool)\n",
        "    alpha_channel = (mask_bool * 255).astype(np.uint8)\n",
        "    cutout_rgba = np.dstack((image_rgb, alpha_channel))\n",
        "    return cutout_rgba\n",
        "\n",
        "def segment_member(image_bgr, member_name, similarity_threshold=0.45):\n",
        "    \"\"\"\n",
        "    Pipeline ‡∏´‡∏•‡∏±‡∏Å: ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û + ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å ‡∏Ñ‡∏∑‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö\n",
        "    \"\"\"\n",
        "    if member_name not in embeddings_db:\n",
        "        return None, None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö embedding ‡∏Ç‡∏≠‡∏á '{member_name}'\"\n",
        "    \n",
        "    target_embedding = embeddings_db[member_name]\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    img_h, img_w = image_bgr.shape[:2]\n",
        "    \n",
        "    # Step 1: Identify all members\n",
        "    members = identify_all_members(image_bgr, face_analyzer, embeddings_db, similarity_threshold)\n",
        "    \n",
        "    target = None\n",
        "    for m in members:\n",
        "        if m[\"name\"] == member_name:\n",
        "            target = m\n",
        "            break\n",
        "    \n",
        "    if target is None:\n",
        "        return None, None, None, None, f\"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö {member_name} ‡πÉ‡∏ô‡∏†‡∏≤‡∏û\"\n",
        "    \n",
        "    # Step 2: Expand face bbox to body bbox\n",
        "    face_bbox = np.array(target[\"bbox\"])\n",
        "    body_bbox = face_to_body_bbox(face_bbox, (img_h, img_w))\n",
        "    \n",
        "    # Step 3: SAM 3 Segmentation\n",
        "    image_pil = Image.fromarray(image_rgb)\n",
        "    mask = segment_by_box(image_pil, body_bbox.tolist())\n",
        "    \n",
        "    # Step 4: Create outputs\n",
        "    overlay = create_overlay(image_rgb, mask)\n",
        "    cutout = create_cutout(image_rgb, mask)\n",
        "    \n",
        "    # Create annotated image\n",
        "    annotated = image_rgb.copy()\n",
        "    for m in members:\n",
        "        x1, y1, x2, y2 = m[\"bbox\"]\n",
        "        is_target = (m[\"name\"] == member_name)\n",
        "        color_box = (0, 255, 0) if is_target else (200, 200, 200)\n",
        "        thickness = 3 if is_target else 1\n",
        "        cv2.rectangle(annotated, (x1, y1), (x2, y2), color_box, thickness)\n",
        "        if m[\"similarity\"]:\n",
        "            label = f\"{m['name']} ({m['similarity']:.2f})\"\n",
        "            cv2.putText(annotated, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color_box, 2)\n",
        "    \n",
        "    status = f\"‚úÖ Found {member_name} (sim={target['similarity']:.3f})\"\n",
        "    \n",
        "    return overlay, cutout, annotated, mask, status\n",
        "\n",
        "print(\"‚úÖ Integration Pipeline ready!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-6'></a>\n",
        "## Section 6: Gradio UI üé®\n",
        "\n",
        "‡∏™‡∏£‡πâ‡∏≤‡∏á Web Interface ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏£‡∏∞‡∏ö‡∏ö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 6: Gradio UI\n",
        "# =============================================================================\n",
        "\n",
        "IVE_MEMBERS = [\"Wonyoung\", \"Yujin\", \"Gaeul\", \"Liz\", \"Leeseo\", \"Rei\"]\n",
        "\n",
        "def gradio_segment(input_image, member_name):\n",
        "    \"\"\"Gradio callback for image segmentation\"\"\"\n",
        "    if input_image is None:\n",
        "        return None, None, None, \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\"\n",
        "    \n",
        "    image_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
        "    \n",
        "    overlay, cutout, annotated, mask, msg = segment_member(image_bgr, member_name)\n",
        "    \n",
        "    if overlay is None:\n",
        "        return None, None, None, msg\n",
        "    \n",
        "    return annotated, overlay, cutout, msg\n",
        "\n",
        "def gradio_identify_all(input_image):\n",
        "    \"\"\"‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà identify ‡πÑ‡∏î‡πâ\"\"\"\n",
        "    if input_image is None:\n",
        "        return None, \"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\"\n",
        "    \n",
        "    image_bgr = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
        "    members = identify_all_members(image_bgr, face_analyzer, embeddings_db)\n",
        "    \n",
        "    annotated = input_image.copy()\n",
        "    info_lines = []\n",
        "    \n",
        "    for m in members:\n",
        "        x1, y1, x2, y2 = m[\"bbox\"]\n",
        "        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        if m[\"similarity\"]:\n",
        "            label = f\"{m['name']} {m['similarity']:.2f}\"\n",
        "            info_lines.append(f\"{m['name']}: similarity={m['similarity']:.3f}\")\n",
        "        else:\n",
        "            label = m[\"name\"]\n",
        "        cv2.putText(annotated, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "    \n",
        "    return annotated, \"\\n\".join(info_lines)\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface\n",
        "with gr.Blocks(title=\"IVE Segmentation\") as demo:\n",
        "    gr.Markdown(\"# üéØ IVE Member Segmentation with SAM 3\")\n",
        "    gr.Markdown(\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏° IVE ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å ‚Üí ‡∏£‡∏∞‡∏ö‡∏ö segment ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏´‡πâ\")\n",
        "    \n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"üîç Segment Member\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    img_input = gr.Image(label=\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\", type=\"numpy\")\n",
        "                    member_dropdown = gr.Dropdown(\n",
        "                        choices=IVE_MEMBERS,\n",
        "                        value=\"Wonyoung\",\n",
        "                        label=\"‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏°‡∏≤‡∏ä‡∏¥‡∏Å\"\n",
        "                    )\n",
        "                    btn_segment = gr.Button(\"üîç Segment\", variant=\"primary\")\n",
        "                    status_text = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                \n",
        "                with gr.Column(scale=2):\n",
        "                    with gr.Row():\n",
        "                        out_identified = gr.Image(label=\"Identified\")\n",
        "                        out_overlay = gr.Image(label=\"Segmented\")\n",
        "                        out_cutout = gr.Image(label=\"Cutout\")\n",
        "            \n",
        "            btn_segment.click(\n",
        "                fn=gradio_segment,\n",
        "                inputs=[img_input, member_dropdown],\n",
        "                outputs=[out_identified, out_overlay, out_cutout, status_text]\n",
        "            )\n",
        "        \n",
        "        with gr.TabItem(\"üë• Identify All\"):\n",
        "            with gr.Row():\n",
        "                img_input_all = gr.Image(label=\"‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û\", type=\"numpy\")\n",
        "                btn_identify = gr.Button(\"üë• Identify All\", variant=\"primary\")\n",
        "            with gr.Row():\n",
        "                out_all = gr.Image(label=\"All Members\")\n",
        "                out_info = gr.Textbox(label=\"Info\", lines=8)\n",
        "            \n",
        "            btn_identify.click(\n",
        "                fn=gradio_identify_all,\n",
        "                inputs=[img_input_all],\n",
        "                outputs=[out_all, out_info]\n",
        "            )\n",
        "\n",
        "print(\"‚úÖ Gradio UI created!\")\n",
        "print(\"‡∏£‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢: demo.launch(share=True)\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ‡∏£‡∏±‡∏ô Gradio UI\n",
        "demo.launch(share=True)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "<a id='section-7'></a>\n",
        "## Section 7: Video Inference üé¨\n",
        "\n",
        "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÅ‡∏ö‡∏ö Frame-by-Frame ‡∏û‡∏£‡πâ‡∏≠‡∏° Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# =============================================================================\n",
        "# SECTION 7: Video Inference\n",
        "# =============================================================================\n",
        "\n",
        "from collections import deque\n",
        "import time\n",
        "\n",
        "class SimpleTracker:\n",
        "    \"\"\"Simple IoU-based tracker\"\"\"\n",
        "    \n",
        "    def __init__(self, iou_threshold=0.5, max_disappeared=5):\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.max_disappeared = max_disappeared\n",
        "        self.objects = {}\n",
        "        self.disappeared = {}\n",
        "        self.next_object_id = 0\n",
        "    \n",
        "    def compute_iou(self, box1, box2):\n",
        "        x1_1, y1_1, x2_1, y2_1 = box1\n",
        "        x1_2, y1_2, x2_2, y2_2 = box2\n",
        "        xi1 = max(x1_1, x1_2)\n",
        "        yi1 = max(y1_1, y1_2)\n",
        "        xi2 = min(x2_1, x2_2)\n",
        "        yi2 = min(y2_1, y2_2)\n",
        "        if xi2 <= xi1 or yi2 <= yi1:\n",
        "            return 0.0\n",
        "        inter_area = (xi2 - xi1) * (yi2 - yi1)\n",
        "        box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "        box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "        union_area = box1_area + box2_area - inter_area\n",
        "        return inter_area / union_area if union_area > 0 else 0.0\n",
        "    \n",
        "    def update(self, detections):\n",
        "        if len(detections) == 0:\n",
        "            for obj_id in list(self.disappeared.keys()):\n",
        "                self.disappeared[obj_id] += 1\n",
        "                if self.disappeared[obj_id] > self.max_disappeared:\n",
        "                    del self.objects[obj_id]\n",
        "                    del self.disappeared[obj_id]\n",
        "            return self.objects\n",
        "        \n",
        "        if len(self.objects) == 0:\n",
        "            for det in detections:\n",
        "                self.objects[self.next_object_id] = det\n",
        "                self.disappeared[self.next_object_id] = 0\n",
        "                self.next_object_id += 1\n",
        "        else:\n",
        "            object_ids = list(self.objects.keys())\n",
        "            object_bboxes = [self.objects[oid]['bbox'] for oid in object_ids]\n",
        "            detection_bboxes = [d['bbox'] for d in detections]\n",
        "            \n",
        "            iou_matrix = np.zeros((len(object_ids), len(detections)))\n",
        "            for i, obj_box in enumerate(object_bboxes):\n",
        "                for j, det_box in enumerate(detection_bboxes):\n",
        "                    iou_matrix[i, j] = self.compute_iou(obj_box, det_box)\n",
        "            \n",
        "            matched_object_ids = set()\n",
        "            matched_detection_indices = set()\n",
        "            \n",
        "            while True:\n",
        "                max_iou = np.max(iou_matrix)\n",
        "                if max_iou < self.iou_threshold:\n",
        "                    break\n",
        "                max_idx = np.unravel_index(np.argmax(iou_matrix), iou_matrix.shape)\n",
        "                obj_idx, det_idx = max_idx\n",
        "                object_id = object_ids[obj_idx]\n",
        "                self.objects[object_id] = detections[det_idx]\n",
        "                self.disappeared[object_id] = 0\n",
        "                matched_object_ids.add(object_id)\n",
        "                matched_detection_indices.add(det_idx)\n",
        "                iou_matrix[obj_idx, :] = -1\n",
        "                iou_matrix[:, det_idx] = -1\n",
        "            \n",
        "            for object_id in object_ids:\n",
        "                if object_id not in matched_object_ids:\n",
        "                    self.disappeared[object_id] += 1\n",
        "                    if self.disappeared[object_id] > self.max_disappeared:\n",
        "                        del self.objects[object_id]\n",
        "                        del self.disappeared[object_id]\n",
        "            \n",
        "            for i, detection in enumerate(detections):\n",
        "                if i not in matched_detection_indices:\n",
        "                    self.objects[self.next_object_id] = detection\n",
        "                    self.disappeared[self.next_object_id] = 0\n",
        "                    self.next_object_id += 1\n",
        "        \n",
        "        return self.objects\n",
        "\n",
        "class TemporalSmoother:\n",
        "    \"\"\"Temporal smoothing for masks\"\"\"\n",
        "    \n",
        "    def __init__(self, window_size=5):\n",
        "        self.window_size = window_size\n",
        "        self.mask_history = deque(maxlen=window_size)\n",
        "    \n",
        "    def update(self, mask):\n",
        "        mask_float = mask.astype(np.float32)\n",
        "        self.mask_history.append(mask_float)\n",
        "        if len(self.mask_history) > 0:\n",
        "            smoothed = np.mean(self.mask_history, axis=0)\n",
        "            return (smoothed > 0.5).astype(np.uint8)\n",
        "        return mask\n",
        "    \n",
        "    def reset(self):\n",
        "        self.mask_history.clear()\n",
        "\n",
        "print(\"‚úÖ Video processing classes loaded!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def process_video(input_path, output_path, target_member, frame_sampling=5):\n",
        "    \"\"\"\n",
        "    ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ frame-by-frame\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Cannot open video: {input_path}\")\n",
        "    \n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "    print(f\"üìπ Video: {width}x{height} @ {fps}fps, {total_frames} frames\")\n",
        "    \n",
        "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps / frame_sampling, (width, height))\n",
        "    \n",
        "    tracker = SimpleTracker(iou_threshold=0.5, max_disappeared=5)\n",
        "    smoother = TemporalSmoother(window_size=5)\n",
        "    \n",
        "    frame_idx = 0\n",
        "    processed_count = 0\n",
        "    \n",
        "    with tqdm(total=total_frames, desc=\"Processing\") as pbar:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            if frame_idx % frame_sampling == 0:\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                \n",
        "                # Identify members\n",
        "                members = identify_all_members(frame, face_analyzer, embeddings_db)\n",
        "                \n",
        "                # Find target member\n",
        "                target_detections = [m for m in members if m['name'] == target_member]\n",
        "                \n",
        "                # Update tracker\n",
        "                tracked = tracker.update(target_detections)\n",
        "                \n",
        "                # Process each tracked object\n",
        "                output_frame = frame_rgb.copy()\n",
        "                \n",
        "                for obj_id, obj_data in tracked.items():\n",
        "                    bbox = obj_data['bbox']\n",
        "                    \n",
        "                    # Segment\n",
        "                    image_pil = Image.fromarray(frame_rgb)\n",
        "                    body_bbox = face_to_body_bbox(np.array(bbox), (height, width))\n",
        "                    mask = segment_by_box(image_pil, body_bbox.tolist())\n",
        "                    \n",
        "                    # Temporal smoothing\n",
        "                    smoothed_mask = smoother.update(mask)\n",
        "                    \n",
        "                    # Visualization\n",
        "                    color = (255, 105, 180)\n",
        "                    output_frame = create_overlay(output_frame, smoothed_mask, color, 0.5)\n",
        "                    \n",
        "                    # Draw bbox\n",
        "                    x1, y1, x2, y2 = bbox\n",
        "                    cv2.rectangle(output_frame, (x1, y1), (x2, y2), color, 2)\n",
        "                    cv2.putText(output_frame, target_member, (x1, y1-10), \n",
        "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "                \n",
        "                # Write output\n",
        "                output_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "                out.write(output_bgr)\n",
        "                processed_count += 1\n",
        "            \n",
        "            frame_idx += 1\n",
        "            pbar.update(1)\n",
        "    \n",
        "    cap.release()\n",
        "    out.release()\n",
        "    \n",
        "    print(f\"\\n‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {processed_count} frames\")\n",
        "    print(f\"   ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏µ‡πà: {output_path}\")\n",
        "    \n",
        "    return output_path\n",
        "\n",
        "print(\"‚úÖ Video processing function ready!\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠)\n",
        "# input_video = \"path/to/video.mp4\"\n",
        "# output_video = \"outputs/segmented_wonyoung.mp4\"\n",
        "# process_video(input_video, output_video, \"Wonyoung\", frame_sampling=5)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üéâ ‡∏™‡∏£‡∏∏‡∏õ\n",
        "\n",
        "‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢:\n",
        "1. ‚úÖ Environment Setup - ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á dependencies\n",
        "2. ‚úÖ Face Embedding Database - ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IVE\n",
        "3. ‚úÖ Identity Matching - ‡∏£‡∏∞‡∏ö‡∏∏‡∏ï‡∏±‡∏ß‡∏ï‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Hungarian Algorithm\n",
        "4. ‚úÖ SAM 3 Engine - Segmentation ‡∏î‡πâ‡∏ß‡∏¢ box prompt\n",
        "5. ‚úÖ Integration Pipeline - ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
        "6. ‚úÖ Gradio UI - Web interface ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô\n",
        "7. ‚úÖ Video Inference - ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏û‡∏£‡πâ‡∏≠‡∏° tracking\n",
        "\n",
        "**‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô RTX 6000 (48GB VRAM)!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}